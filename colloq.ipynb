{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ядра. Теорема Мерсера. Способы построения ядер. Полиномиальные и гауссовы ядра.\n",
    "\n",
    "### здесь и далее важные на первом этапе вещи обрамляются конструкцией [Входит в теормин] - []\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Ядром называется функция $K(x,z)$, представимая в виде скалярного произведения в некотором пространстве: $K(x,z) = \\langle \\phi(x), \\phi(z) \\rangle$, где функция $\\phi: X \\rightarrow H$ является отображением из исходного пространства в некоторое спрямляющее пространство.\n",
    "\n",
    "Также можно определить ядро с помощью теоремы Мерсера: функция $K(x, z)$ является ядром $\\iff$\n",
    "\n",
    "1. $K(x, z) = K(z, x)$ - функция симметрична\n",
    "2. Для любой конечной выборки $(x_1,..., x_\\ell)$ матрица $K = \\left(K(x_i, x_j)\\right)_{i,j=1}^{\\ell}$ неотрицательно определена - функция неотрицательно определена\n",
    "\n",
    "**[]**\n",
    "\n",
    "Пользоваться этим на практике сложно, поэтому для построения ядер используются некоторые базовые вещи:\n",
    "\n",
    "Пусть нам даны ядра $K_1(x, z), K_2(x, z)$ на пространстве $X$, $f: X \\to \\mathbb{R}$ - вещественная функция на $X$, $\\phi: X \\to \\mathbb{R}^N$ - векторная функция на $X$ и $K_3$ - ядро на пространстве $\\mathbb{R}^N$. Тогда ядро можно получить с помощью следующих операций:\n",
    "\n",
    "1. $K(x, z) = K_1(x, z) + K_2(x, z)$\n",
    "\n",
    "2. $K(x, z) = \\alpha K_1(x, z), \\alpha > 0$\n",
    "\n",
    "3. $K(x,z) = K_1(x, z) \\cdot K_2(x, z)$\n",
    "\n",
    "4. $K(x,z) = f(x) \\cdot f(z)$\n",
    "\n",
    "5. $K(x,z) = K_3(\\phi(x), \\phi(z))$\n",
    "\n",
    "Пусть $K_1(x, z), K_2(x, z)...$ - последовательность ядер, причем $ K(x,z) = \\lim_{n \\to \\infty} K_n(x,z)$ $\\exists \\forall x, z$. Тогда $K(x,z)$ - ядро.\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Пусть $p(v)$ - многочлен с положительными коэффициентами. Очевидно, $p(K(x,z))$ является ядром для любого $K(x,z)$. \n",
    "\n",
    "Полиномиальное ядро степени $m$ можно определить как $K_m(x,z) = \\left(\\langle x,z \\rangle + R \\right)^m$. Согласно формуле бинома, \n",
    "\n",
    "$$K_m(x,z) = \\sum_{i=0}^m C_m^i R^{m-i} \\langle x,z \\rangle^i$$\n",
    "\n",
    "Все коэффициенты положительны, поэтому это действительно ядро. Оно соответствует переводу признаков во все возможные наборы мономов над признаками степени не более $m$.\n",
    "\n",
    "___________\n",
    "\n",
    "Гауссово ядро определяется как $K(x,z) = \\exp{\\left(-\\frac{||x-z||^2}{2\\sigma^2} \\right)}$. Докажем, что это действительно ядро:\n",
    "\n",
    "$$\\exp{(\\langle x,z \\rangle)} = \\sum_{k=0}^{\\infty} \\frac{\\langle x,z \\rangle^k}{k!} = \\lim_{n \\to \\infty} \\sum_{k=0}^{n} \\frac{\\langle x,z \\rangle^k}{k!}$$\n",
    "\n",
    "Каждый член этой числовой последовательности - ядро как многочлен с положительными коэффициентами, предел существует во всех точках (т.к. ряд Тейлора экспоненты сходится всюду) - отсюда получаем, что экспонента скалярного произведения является ядром.\n",
    "\n",
    "Аналогично доказывается ядерность $\\exp{\\left(\\frac{\\langle x,z \\rangle}{\\sigma^2}\\right)}$.\n",
    "\n",
    "Осталось перейти к исходному ядру, для этого сделаем так\n",
    "\n",
    "$$||x - z||^2 = \\langle x,x \\rangle + \\langle z,z \\rangle -2 \\langle x,z \\rangle$$\n",
    "\n",
    "$$\\exp{(-||x - z||^2)} = \\frac{\\exp(2\\langle x,z \\rangle)}{\\exp(||x||^2) \\exp(||z||^2)}$$\n",
    "\n",
    "Сверху ядро, снизу произведение вещественных функций - ядро, общее произведение - ядро.\n",
    "\n",
    "Это ядро задает бесконечномерное полиномиальное пространство (доказывается Тейлором).\n",
    "\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Задача условной оптимизации. Двойственная задача. Теорема Куна-Таккера.\n",
    "\n",
    "Запишем некоторую задачу оптимизации с ограничениями:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^d} \\\\\n",
    "\\\\\n",
    "f_i(x) \\leq 0, & \\forall i \\in \\{1,...,m\\} \\\\\n",
    "\\\\\n",
    "h_i(x) = 0, & \\forall i \\in \\{1,...,p\\} \\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Собственно, это и есть задача условной оптимизации.\n",
    "\n",
    "Из нее можно получить задачу безусловной оптимизации:\n",
    "\n",
    "$$f_0(x) + \\sum\\limits_{i=1}^{m} I\\_(f_i(x)) + \\sum\\limits_{i=1}^{p} I_0(h_i(x)) \\to \\min\\limits_x$$\n",
    "\n",
    "$$I\\_(x) = \\begin{align}\\begin{cases} 0, & x \\leq 0 \\\\ \\infty, & x > 0 \\end{cases}\\end{align}$$\n",
    "\n",
    "$$I_0(x) = \\begin{align}\\begin{cases} 0, & x = 0 \\\\ \\infty, & x \\neq 0 \\end{cases}\\end{align}$$\n",
    "\n",
    "Но это сложно оптимизировать, поэтому заменим индикаторы на линейные аппроксимации:\n",
    "\n",
    "$$L(x, \\lambda, \\mu) = f_0(x) + \\sum\\limits_{i=1}^{m} \\lambda_i f_i(x) + \\sum\\limits_{i=1}^{p} \\mu_i h_i(x)$$\n",
    "\n",
    "Выражение выше называется лагранжианом исходной задачи, а новые коэффициенты $\\lambda, \\mu$ - двойственными переменными.\n",
    "\n",
    "Теперь мы можем выразить через него двойственную функцию исходной задачи:\n",
    "\n",
    "$$g(\\lambda, \\mu) = \\inf\\limits_x L(x, \\lambda, \\mu)$$\n",
    "\n",
    "Эта функция дает нижнюю оценку на минимум в исходной задаче.\n",
    "\n",
    "$$g(\\lambda, \\mu) \\leq f_0(x_*)$$\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Соответственно, двойственную к исходной задачу можно представить как максимизацию двойственной функции с ограничениями на новые коэффициенты:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "g(\\lambda, \\mu) \\to \\max\\limits_{\\lambda, \\mu} \\\\\n",
    "\\lambda_i \\geq 0,& i \\in \\{1,...,m\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "**[]**\n",
    "\n",
    "Очевидно, для любого решения двойственной задачи $(\\lambda^*, \\mu^*)$ значение двойственной функции не превосходит условный минимум исходной задачи: $$g(\\lambda^*, \\mu^*) \\leq f_0(x^*)$$\n",
    "\n",
    "Это свойство называется слабой двойственностью, в случае равенства имеет место сильная двойственность. Разность между правой и левой частью называется двойственным зазором.\n",
    "\n",
    "Для сильной двойственности в выпуклых задачах имеются различные достаточные условия. Сформулируем одно из них.\n",
    "\n",
    "Задача оптимизации называется выпуклой, если она имеет следующий вид:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^d}\\\\\n",
    "f_i(x) \\leq 0,& i \\in \\{1,...,m\\} \\\\\n",
    "Ax = b \\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "где функции $f_0, f_1,..., f_m$ являются выпуклыми. Условие Слейтера требует существование такой точки $x'$, в которой все ограничения бы выполнялись, при этом ограничения-неравенства были бы выполнены строго. Впрочем, достаточно, чтобы эти неравенства выполнялись строго, только если они имеют нелинейный вид.\n",
    "\n",
    "Пусть у нас есть решения прямой и двойственной задач $x_*$ и $(\\lambda^*, \\mu^*)$ соответственно, и пусть имеет место сильная двойственность. В таком случае $$f_0(x_*) = g(\\lambda^*, \\mu^*) = \\inf\\limits_{x} \\left( f_0(x) + \\sum\\limits_{i=1}^{m} \\lambda^*_i f_i(x) + \\sum\\limits_{i=1}^p \\mu^*_i h_i(x) \\right) \\leq $$ $$ \\leq f_0(x_*) + \\sum\\limits_{i=1}^{m} \\lambda^*_i f_i(x_*) + \\sum\\limits_{i=1}^p \\mu^*_i h_i(x_*) \\leq f_0(x_*)$$\n",
    "\n",
    "Все неравенства здесь превращаются в равенства. Отсюда мы можем получить, что $\\lambda_i^* f_i(x_*) = 0$ $\\forall i \\in \\{1,...,m\\}$ (следует из равенства суммы нулю и неположительности каждого слагаемого). Эти условия называются условиями дополняющей нежесткости, согласно им множитель $\\lambda$ при $i-$ом ограничении может быть ненулевым, только если оно выполняется с равенством (является активным). \n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Запишем условия, выполненные для решения прямой и двойственной задач:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\nabla f_0(x_*) + \\sum\\limits_{i=1}^m \\lambda_i^* \\nabla f_i(x_*) + \\sum\\limits_{i=1}^p \\mu_i^* \\nabla h_i(x_*) = 0 \\\\\n",
    "f_i(x_*) \\leq 0,& i \\in \\{1,...,m\\}\\\\\n",
    "h_i(x_*) = 0,& i \\in \\{1,...,p\\}\\\\\n",
    "\\lambda_i^* \\geq 0,& i \\in \\{1,...,m\\}\\\\\n",
    "\\lambda_i^* f_i(x_*) = 0,& i \\in \\{1,...,m\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Эти условия называются условиями Каруша-Куна-Таккера и являются необходимыми условиями экстремума. Можно сформулировать в виде теоремы: пусть есть решение $x_*$, тогда найдутся такие $\\lambda^*, \\mu^*$, что выполнены условия ККТ. Если задача является выпуклой и удовлетворяет условиям Слейтера, то условия ККТ будут необходимыми и достаточными. \n",
    "\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Двойственная задача SVM (с выводом). Какие объекты называются периферийными, опорными граничными, опорными нарушителями?\n",
    "\n",
    "Исходная задача SVM формулируется так:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{1}{2}||w||^2 + C \\sum\\limits_{i=1}^\\ell \\xi_i \\to \\min\\limits_{w,b,\\xi}\\\\\n",
    "y_i(\\langle w, x_i \\rangle + b) \\geq 1 - \\xi_i,& i \\in \\{1,...,\\ell\\}\\\\\n",
    "\\xi_i \\geq 0,& i \\in \\{1,...,\\ell\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Запишем ее лагранжиан:\n",
    "\n",
    "$$L(w,b,\\xi,\\lambda,\\mu) = \\frac{1}{2}||w||^2 + C \\sum\\limits_{i=1}^\\ell \\xi_i - \\sum\\limits_{i=1}^\\ell \\lambda_i \\left[y_i (\\langle w, x_i \\rangle + b) - 1 + \\xi_i \\right] - \\sum\\limits_{i=1}^\\ell \\mu_i \\xi_i$$\n",
    "\n",
    "Сформулируем условия Куна-Таккера:\n",
    "\n",
    "1. $\\nabla_w L = w - \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i = 0 \\Longrightarrow w = \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i$\n",
    "\n",
    "\n",
    "2. $\\nabla_b L = - \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i = 0 \\Longrightarrow \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i = 0$ \n",
    "\n",
    "\n",
    "3. $\\nabla_{\\xi_i} L = C - \\lambda_i - \\mu_i = 0 \\Longrightarrow \\lambda_i + \\mu_i = C$\n",
    "\n",
    "\n",
    "4. $\\lambda_i \\left[y_i (\\langle w, x_i \\rangle + b) - 1 + \\xi_i \\right] = 0\\Longrightarrow \n",
    "(\\lambda_i = 0)$ или $ (y_i (\\langle w, x_i \\rangle + b) = 1 - \\xi_i) $\n",
    "\n",
    "\n",
    "5. $\\mu_i \\xi_i = 0 \\Longrightarrow (\\mu_i = 0)$ или $(\\xi_i = 0)$\n",
    "\n",
    "\n",
    "6. $\\lambda_i \\geq 0, \\mu_i \\geq 0, \\xi_i \\geq 0$\n",
    "\n",
    "Здесь можно выделить три случая на основании значений $\\xi, \\lambda$:\n",
    "\n",
    "1. $\\xi_i = 0, \\lambda_i = 0$ - в таком случае объект не участвует в формировании вектора весов (входит в сумму с нулевым весом) и при этом верно классифицируется (штраф 0). Назовем его периферийным.\n",
    "\n",
    "\n",
    "2. $\\xi_i = 0, 0 < \\lambda_i \\leq C$ - в таком случае объект верно классифицируется (штраф 0), при этом он находится на границе разделяющей полосы (отступ ровно 1) и дает ненулевой вклад в вектор весов. Назовем такой объект опорным граничным.\n",
    "\n",
    "\n",
    "3. $\\xi_i > 0, \\lambda_i = C$ - в таком случае объект лежит либо внутри разделяющей полосы ($0 < \\xi_i < 2$, при этом при $\\xi_i < 1$ он классифицируется верно, иначе неверно), либо выходит за ее пределы (и классифицируется неверно) и дает ненулевой вклад в вектор весов. Такой объект будем называть опорным нарушителем. \n",
    "\n",
    "Заметим, что эти случаи покрывают все возможные значения $\\lambda, \\xi$ - к примеру, случая $\\xi_i > 0, \\lambda_i < C$ быть не может в силу условий 5 и 3.\n",
    "\n",
    "Теперь сформулируем двойственную задачу. Сначала нам надо написать двойственную функцию (подставим условия 1-3 в лагранжиан):\n",
    "\n",
    "$$L = \\frac{1}{2} \\left|\\left|\\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i\\right|\\right|^2 - \\sum\\limits_{i,j=1}^{\\ell} \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle - b \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i + \\sum\\limits_{i=1}^{\\ell} \\lambda_i + \\sum_{i=1}^{\\ell} \\xi_i (C - \\lambda_i - \\mu_i) =$$ $$ = \\sum_{i=1}^\\ell \\lambda_i - \\frac{1}{2} \\sum\\limits_{i=1}^\\ell \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle$$\n",
    "\n",
    "Эту функцию нужно будет максимизировать. Ограничения мы введем на неотрицательность двойственных переменных $\\lambda, \\mu$, а также потребуем выполнение условий 2 и 3. Получаем такую систему: \n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\sum_{i=1}^\\ell \\lambda_i - \\frac{1}{2} \\sum\\limits_{i=1}^\\ell \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle \\to \\max\\limits_\\lambda \\\\\n",
    "\\\\\n",
    "0 \\leq \\lambda_i \\leq C,& i \\in \\{1,..., \\ell\\} \\\\\n",
    "\\\\\n",
    "\\sum\\limits_{i=1}^\\ell \\lambda_i y_i = 0\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Аппроксимация спрямляющего пространства: метод случайных признаков Фурье.\n",
    "\n",
    "Чтобы избавиться от довольно затратного хранения матрицы Грама для признакового описания выборки, попробуем в явном виде построить преобразование $\\tilde{\\phi}(x)$ пространства в пространство небольшой размерности, в котором уже можно обучать модели напрямую. Для этого используем метод случайных признаков Фурье (Random Kitchen Sinks). Его особенность в том, что он может аппроксимировать скалярное произведение, т.е. $\\langle \\tilde{\\phi}(x), \\tilde{\\phi}(z) \\rangle \\approx K(x,z)$\n",
    "\n",
    "Из комплексного анализа известно (кому это блять известно), что любое непрерывное ядро вида $K(x,z) = K(x-z)$ является преобразованием Фурье некоторого вероятностного распределения: $$K(x-z) = \\int_{\\mathbb{R}^d} p(w) e^{iw^T(x-z)} dw = \\int_{\\mathbb{R}^d} p(w) \\cos\\left(w^T (x-z)\\right)dw + i\\int_{\\mathbb{R}^d} p(w) \\sin\\left(w^T (x-z)\\right)dw$$\n",
    "\n",
    "Поскольку значение ядра всегда вещественное, мнимую часть интеграла можно приравнять к нулю. Вещественную часть (с косинусом) приблизим через Монте-Карло:\n",
    "\n",
    "$$\\int_{\\mathbb{R}^d} p(w) \\cos\\left(w^T (x-z)\\right)dw \\approx \\frac{1}{n} \\sum\\limits_{j=1}^{n} \\cos\\left(w_j^T(x-z)\\right)$$\n",
    "\n",
    "Здесь векторы $w_1,...,w_n$ сэмплируются из вероятностного распределения $p(w)$. Используя эти векторы, можем аппроксимировать искомое преобразование: \n",
    "\n",
    "$$\\phi(x) \\approx \\tilde{\\phi}(x) =  \\frac{1}{\\sqrt{n}} \\left(\\cos(w_1^Tx),...,\\cos(w_n^Tx),\\sin(w_1^T(x),...,\\sin(w_n^Tx)\\right)$$\n",
    "\n",
    "В таком случае скалярное произведение новых признаков будет иметь вид $$\\tilde{K}(x,z) = \\langle \\tilde{\\phi}(x), \\tilde{\\phi}(z) \\rangle = \\frac{1}{n} \\sum\\limits_{j=1}^n \\left(\\cos(w_j^Tx)\\cos(w_j^Tz) + \\sin(w_j^Tx)\\sin(w_j^Tz) \\right) = $$ $$ = \\frac{1}{n} \\sum\\limits_{j=1}^n \\cos\\left(w_j^T(x-z)\\right)$$\n",
    "\n",
    "Данная аппроксимация является несмещенной оценкой для $K(x,z)$. Чаще всего данный метод используется для Гауссовых ядер, поскольку для них распределение $p(w)$ довольно просто найти - оно будет нормальным со средним 0 и дисперсией $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Скрытые переменные, полное и неполное правдоподобие на примере смеси распределений. EM-алгоритм (описание шагов).\n",
    "\n",
    "Распределение $p(x)$ назовем смесью распределений, если его плотность имеет вид $$p(x) = \\sum\\limits_{k=1}^K p_k(x) \\pi_k(x), $$ $$\\sum\\limits_{k=1}^K \\pi_k = 1, \\pi_k \\geq 0,$$\n",
    "\n",
    "где $p_k(x)$ - параметризованные распределения компонент смеси $\\left(p_k(x) = \\phi(x|\\theta_k)\\right)$, $\\pi_k$ - априорные вероятности этих компонент, $K$ - их число. \n",
    "\n",
    "(не знаю, нужно ли это рассказывать, но в целом далее идет тривиальное рукомахание)\n",
    "\n",
    "Предположим, мы сэмплируем $x$ следующим образом - сначала из дискретного распределения $\\{\\pi_1,...,\\pi_k\\}$ выбирается номер компоненты $k$, а затем из распределения с плотностью $\\phi(x|\\theta_k)$ сэмплируется $x$. В таком случае распределение $x$ описывается вышеупомянутой смесью. \n",
    "\n",
    "Чтобы показать это, добавим в модель скрытую переменную $z$, которая будет являться $K$-мерным случайным one-hot вектором, отвечающим за выбор компоненты в смеси. При этом вектор сэмплируется из априорного распределения $\\pi$, т.е. $p(z_k = 1) = \\pi_k$. В таком случае распределение всего вектора выглядит так: $$p(z) = \\prod\\limits_{k=1}^k \\pi_k^{z_k}$$ \n",
    "\n",
    "Если мы уже знаем номер компоненты, то можем легко описать распределение $x$:\n",
    "\n",
    "$$p(x|z_k = 1) = \\phi(x|\\theta_k)$$ \n",
    "\n",
    "И обобщить это следующим образом:\n",
    "\n",
    "$$p(x|z) = \\prod\\limits_{k=1}^K \\left[\\phi(x|\\theta_k)\\right]^{z_k}$$ \n",
    "\n",
    "Отсюда выражается совместное распределение $x$ и $z$:\n",
    "\n",
    "$$p(x,z) = p(z)p(x|z) = \\prod\\limits_{k=1}^K \\left[\\pi_k \\phi(x|\\theta_k)\\right]^{z_k}$$ \n",
    "\n",
    "Чтобы найти распределение $x$, нужно маргинализировать совместное распределение по $z$, т.е. просуммировать по всем возможным one-hot векторам:\n",
    "\n",
    "$$p(x) = \\sum_z p(x,z) = \\sum_z \\left(\\prod\\limits_{k=1}^K \\left[\\pi_k \\phi(x|\\theta_k)\\right]^{z_k}\\right) = \\sum_{k=1}^K \\pi_k \\phi(x|\\theta_k)$$ \n",
    "\n",
    "(теперь о серьезных вещах)\n",
    "\n",
    "Допустим, у нас есть вероятностная модель с наблюдаемыми переменными $X$ и параметрами $\\Theta$, для которой задана функция лог-правдоподобия $\\log p(X|\\Theta)$. Допустим также, что в этой модели имеются скрытые переменные $Z$, которые описывают ее внутреннее состояние. Тогда будем называть правдоподобие наблюдаемых переменных $\\log p(X|\\Theta)$ неполным, а совместное правдоподобие наблюдаемых и скрытых переменных $\\log p(X,Z|\\Theta)$ - полным. Неполное правдоподобие можно получить из полного маргинализацией по скрытым переменным: $$\\log p(X|\\Theta) = \\log{\\left[ \\sum_Z p(X,Z|\\Theta)\\right]}$$ \n",
    "\n",
    "Рассмотрим эту модель для смеси распределений. Наблюдаемые переменные $X$ - это наша выборка $\\{x_1,...,x_\\ell \\}$. Скрытые переменные $Z$ - номера компонент $\\{z_1,...,z_\\ell\\}$ (каждый представлен one-hot вектором), из которых сэмплированы объекты выборки. Параметрами $\\Theta = (\\theta_1,...,\\theta_K,\\pi_1,...,\\pi_k)$ же будут являться априорные вероятности выбора компонент и параметры распределения каждой компоненты.\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Неполное правдоподобие этой модели выражается так:\n",
    "\n",
    "$$\\log p(X|\\Theta) = \\sum\\limits_{i=1}^\\ell \\log{\\left[ \\sum\\limits_{k=1}^K \\pi_k \\phi(x_i | \\theta_k) \\right]}$$ \n",
    "\n",
    "Оно крайне неудобно для оптимизации, поскольку является логарифмом суммы. Поэтому перейдем к полному правдоподобию:\n",
    "\n",
    "$$\\log p(X,Z|\\Theta) = \\sum\\limits_{i=1}^\\ell \\sum\\limits_{k=1}^K z_{ik} \\left[\\log \\pi_k + \\log \\phi(x_i | \\theta_k) \\right]$$ \n",
    "**[]**\n",
    "\n",
    "Это уже удобнее оптимизировать, но есть такая проблема, что для нахождения ОМП на $\\Theta$ нам должны быть известны скрытые переменные $Z$. Они нам неизвестны, поэтому придется оценивать их одновременно с параметрами. Отсюда и вытекает ЕМ-алгоритм.\n",
    "\n",
    "В сущности, мы хотим максимизировать полное правдоподобие, попеременно находя наиболее вероятные значения для $Z$ и для $\\Theta$. Получается, что мы будем чередовать следующие шаги: \n",
    "\n",
    "$$ Z^* = \\arg\\max_Z p(X, Z|\\Theta^{old}) $$ \n",
    "\n",
    "$$\\Theta^{new} = \\arg\\max_\\Theta p(X, Z^* | \\Theta)$$ \n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Теперь, сформулируем более четко. \n",
    "\n",
    "На Е-шаге (Expectation) мы выбираем наиболее вероятное распределение скрытых переменных $Z$ - используем байесовский подход и вычислим здесь апостериорное распределение при фиксированных параметрах $p(Z|X,\\Theta^{old})$.\n",
    "\n",
    "На М-шаге (Maximization) мы подбираем такие параметры $\\Theta$, которые максимизируют ожидание полного лог-правдоподобия по апостериорному распределению $Z$:\n",
    "\n",
    "$$Q(\\Theta, \\Theta^{old}) = \\mathbb{E}_{Z \\sim p(Z|X,\\Theta^{old})} \\log p(X,Z|\\Theta) = \\sum_Z p(Z|X, \\Theta^{old}) \\log p(X,Z|\\Theta)$$ \n",
    "\n",
    "$$\\Theta^{new} = \\arg\\max_\\Theta Q(\\Theta, \\Theta^{old}) = \\arg\\max_\\Theta \\sum_Z p(Z|X, \\Theta^{old}) \\log p(X,Z|\\Theta)$$\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Дивергенция Кульбака-Лейблера, её неотрицательность. Вывод E- и M-шагов через разложение логарифма неполного правдоподобия на нижнюю оценку и KL-дивергенцию.\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "KL-дивергенция является мерой расстояния между двумя распределениями. Формулируется она так: \n",
    "\n",
    "$$\\text{KL}(q || p) = \\int_{\\mathbb{R}} q(x) \\log \\frac{q(x)}{p(x)} dx$$\n",
    "\n",
    "(здесь интеграл берется по всему пространству)\n",
    "\n",
    "В дискретном случае: \n",
    "\n",
    "$$\\text{KL}(q || p) = \\sum_{x} q(x) \\log \\frac{q(x)}{p(x)}$$\n",
    "**[]**\n",
    "\n",
    "При вычислении полагаем, что все неопределенности обращаются в 0. Эта мера определена только в том случае, если из $p(x) = 0$ следует $q(x) = 0$.\n",
    "\n",
    "Можно заметить, что данная мера является асимметричной и неотрицательной. Неотрицательность можно доказать, если использовать неравенство $\\log(x) \\leq x - 1$. В таком случае $$\\text{KL}(q || p) = -\\sum_{x} q(x) \\log \\frac{p(x)}{q(x)} \\leq \\sum_{x} q(x) \\left( \\frac{p(x)}{q(x)} - 1\\right) = \\sum_x p(x) - \\sum_x q(x) = 1 - 1 = 0$$\n",
    "\n",
    "**[Входит в теормин, не входит в основную часть))]**\n",
    "\n",
    "Пусть нам дана выборка $X^\\ell$ и распределение $p(x|\\theta)$, параметры которого мы хотим настроить под данную выборку. Эмпирическим распределением называется дискретное распределение на объектах, которое присваивает равные вероятности $\\frac{1}{\\ell}$ всем объектам из обучающей выборки: $$\\hat{p}\\left(x | X^\\ell\\right) = \\sum\\limits_{i=1}^\\ell \\frac{1}{\\ell} [x = x_i]$$. \n",
    "\n",
    "Максимизация правдоподобия эквивалентна минимизации $\\text{KL}\\left(\\hat{p}\\left(x | X^\\ell\\right) || p(x|\\theta)\\right)$ - KL-дивергенции между эмпирическим и модельным распределением. Докажем.\n",
    "\n",
    "$$\\text{KL}\\left(\\hat{p}\\left(x | X^\\ell\\right) || p(x|\\theta)\\right) = \\sum\\limits_{i=1}^\\ell \\frac{1}{\\ell} \\log \\frac{\\frac{1}{\\ell}}{p(x_i|\\theta)} = $$ $$ = \\sum\\limits_{i=1}^\\ell \\frac{1}{\\ell} \\log \\frac{1}{\\ell} - \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell \\log p(x_i|\\theta) \\to \\min_\\theta$$\n",
    "\n",
    "При отбрасывании константных членов видим, что это равносильно $\\log p(x_i|\\theta) \\to \\max\\limits_\\theta$ - задаче максимизации правдоподобия.\n",
    "\n",
    "**[]**\n",
    "\n",
    "С помощью KL-дивергенции мы можем вывести шаги ЕМ-алгоритма. Для начала разложим неполное лог-правдоподобие на сумму двух функций:\n",
    "\n",
    "$$\\log p(X|\\Theta) = \\mathcal{L}(q, \\Theta) + \\text{KL}(q||p)$$\n",
    "\n",
    "$$\\mathcal{L}(q, \\Theta) = \\sum_Z q(Z) \\log \\frac{p(X,Z|\\Theta)}{q(Z)}$$\n",
    "\n",
    "$$\\text{KL}(q||p) = -\\sum_Z q(Z) \\log \\frac{p(Z|X, \\Theta)}{q(Z)}$$\n",
    "\n",
    "Здесь $q(Z)$ - произвольное распределение на скрытых переменных. \n",
    "\n",
    "Разложение корректно, так как \n",
    "\n",
    "$$\\sum_Z q(Z) \\log \\frac{p(X,Z|\\Theta)}{q(Z)} - \\sum_Z q(Z) \\log \\frac{p(Z|X, \\Theta)}{q(Z)} = $$\n",
    "\n",
    "$$ = \\sum_Z q(Z) \\log \\frac{p(X,Z|\\Theta)}{p(Z|X,\\Theta)} = \\sum_Z q(Z) \\log p(X|\\Theta) = $$ \n",
    "\n",
    "$$ = \\log p(X|\\theta) \\sum_Z q(Z) = \\log p(X|\\theta)$$\n",
    "\n",
    "В силу неотрицательности KL-дивергенции можем сказать, что $\\mathcal{L}(q, \\Theta)$ будет являться нижней оценкой для неполного лог-правдоподобия $\\log p(X|\\Theta)$. Можно сказать, что чем корректнее выбрано распределение $q$, тем точнее будет нижняя оценка. \n",
    "\n",
    "Будем по очереди максимизировать эту нижнюю оценку по $q$ и по $\\Theta$. \n",
    "\n",
    "Зафиксируем параметры $\\Theta^{old}$. Поскольку неполное лог-правдоподобие никак не зависит от $q$, его нижняя оценка будет максимальна в том случае, если KL-дивергенция будет минимальна, то есть $q(Z) = p(Z|X,\\Theta^{old})$. Собственно, это и есть E-шаг алгоритма.\n",
    "\n",
    "Зафиксируем теперь $q(Z)$ и найдем максимум нижней оценки по $\\Theta$. Нам нужно решить следующую задачу оптимизации:\n",
    "\n",
    "$$\\mathcal{L}(q, \\Theta) = \\sum_Z q(Z) \\log \\frac{p(X,Z|\\Theta)}{q(Z)} = \\sum_Z q(Z) \\log p(X,Z|\\Theta) - \\sum_Z q(Z) \\log q(Z) =$$\n",
    "\n",
    "$$ = \\sum_Z p(Z|X, \\Theta^{old}) \\log p(X,Z|\\Theta) + \\text{const}(\\Theta) = Q(\\Theta, \\Theta^{old}) + \\text{const}(\\Theta) \\to \\max_\\Theta$$\n",
    "\n",
    "Именно этим мы и занимаемся на М-шаге.\n",
    "\n",
    "Отметим, что такое разложение позволяет нам показать важное свойство ЕМ-алгоритма - на каждой итерации значение его правдоподобия как минимум не уменьшается. Действительно, \n",
    "\n",
    "$$\\log p(X, \\Theta^{new}) = \\mathcal{L} (q, \\Theta^{new}) + \\text{KL}(q||p) \\geq \\mathcal{L} (q, \\Theta^{new}) \\geq \\mathcal{L} (q, \\Theta^{old}) = \\log p(X, \\Theta^{old}),$$\n",
    "\n",
    "так как после Е-шага мы занулили KL-дивергенцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Лапласиан графа, его свойства. Связь кратности нулевого собственного значения с числом компонент связности (с доказательством). Алгоритм спектральной кластеризации. \n",
    "\n",
    "Допустим, нам дана выборка $\\{X_1,...,X_\\ell\\}$ и нам захотелось представить ее в виде неориентированного графа. В таком случае мы можем задать вершины графа как объекты этой выборки, а ребра подкостылить следующими эвристиками:\n",
    "\n",
    "1. Использовать полный граф, в котором веса ребер определяются некоторой формулой. E.g.\n",
    "\n",
    "$$w_{ij} = \\exp \\left(- \\frac{||x_i - x_j||^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "$\\sigma$ отвечает за то, насколько нам важны далекие объекты.\n",
    "\n",
    "2. Использовать k-NN граф. В нем каждая вершина будет соединена ребрами с $k$ ближайших соседей.\n",
    "\n",
    "3. Использовать $\\epsilon$-граф. В нем каждая вершина будет соединена с теми вершинами, расстояние до которых не превосходит некоторого $\\epsilon$. \n",
    "\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Пусть нам дан граф $G$, имеющий матрицу смежности $W$. Степени вершин в таком графе будем обозначать как $d_i = \\sum_{j=1}^{n} w_{ij}$. Зададим матрицу $D = \\text{diag}(d_1,...,d_n)$ - диагональная матрица из степеней вершин. В таком случае лапласианом графа $G$ будет называться матрица $L = D - W$.\n",
    "\n",
    "Рассмотрим некоторые свойства лапласиана.\n",
    "\n",
    "1. Он симметричен. Поскольку мы работаем с вышеобозначенной задачей, в которой граф неориентирован, это действительно так.\n",
    "\n",
    "2. Пусть $f \\in \\mathbb{R}^n$. Тогда справедлива следующая формула: $$f^T L f = \\frac{1}{2} \\sum\\limits_{i,j=1}^{n} w_{ij} (f_i - f_j)^2$$ Док-во:\n",
    "\n",
    "$$f^T L f = f^T D f - f^T W f = \\sum\\limits_{i=1}^n d_i f_i^2 - \\sum\\limits_{i,j=1}^n w_{ij} f_i f_j = $$\n",
    "\n",
    "$$ = \\sum\\limits_{i=1}^n \\left(\\sum\\limits_{i,j=1}^{n} w_{ij} \\right) f_i^2 - \\sum\\limits_{i,j=1}^n w_{ij} f_i f_j = \\sum\\limits_{i,j=1}^n w_{ij} f_i^2 - \\sum\\limits_{i,j=1}^n w_{ij} f_i f_j = $$ \n",
    "\n",
    "$$ = \\sum\\limits_{i,j=1}^n w_{ij} (f_i^2 - f_i f_j) = [\\text{symmetric}] = \\sum\\limits_{i=1}^n \\sum\\limits_{j=i+1}^n w_{ij} (f_i^2 - 2 f_i f_j + f_j^2) = $$\n",
    "\n",
    "$$ = \\sum\\limits_{i=1}^n \\sum\\limits_{j=i+1}^n w_{ij} (f_i -  f_j)^2 = \\frac{1}{2} \\sum\\limits_{i,j=1}^n w_{ij} (f_i^2 - f_i f_j)$$\n",
    "\n",
    "3. Он неотрицательно определен. В самом деле, в формуле выше каждое слагаемое является произведением двух неотрицательных элементов, в таком случае $f^T L f \\geq 0$  $\\forall f$ - что и означает неотрицательную определенность.\n",
    "\n",
    "**[]**\n",
    "\n",
    "Кроме того, у лапласиана есть свойство, которое позволяет использовать его для задачи кластеризации. Пусть $L$ - лапласиан графа $G$. Тогда мы можем сказать следующее:\n",
    "\n",
    "1. Нулевое собственное значение лапласиана имеет кратность $k$, равную числу компонент связности в графе.\n",
    "\n",
    "2. Пусть $A_1,...,A_k$ - компоненты связности графа $G$. Тогда векторы $f_1,...,f_k$ вида $f_i = ([x_j \\in A_i])_{j=1}^{\\ell}$ - индикаторные векторы вхождения объектов в данную компоненту связности для каждой компоненты - будут являться собственными векторами для нулевого собственного значения.\n",
    "\n",
    "Докажем это.\n",
    "\n",
    "Сперва рассмотрим случай $k = 1$. Покажем, что для матрицы $L$ $\\lambda = 0$ будет собственным значением. Для этого нам нужно рассмотреть вектор $f = (1,...,1)$. \n",
    "\n",
    "$$Lf = Df - Wf = \\begin{pmatrix}d_1 & 0 & \\cdots & 0\\\\ 0 & d_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & d_{\\ell} \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} - \\begin{pmatrix}w_{11} & w_{12} & \\cdots & w_{1\\ell}\\\\ w_{12} & w_{22} & \\cdots & w_{2\\ell} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{\\ell 1} & w_{\\ell 2} & \\cdots & w_{\\ell\\ell} \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} = $$\n",
    "\n",
    "\n",
    "$$ = \\begin{pmatrix} d_1 \\\\ \\vdots \\\\ d_\\ell\\end{pmatrix} - \\begin{pmatrix} w_{11} + \\cdots + w_{1\\ell} \\\\ \\vdots \\\\ w_{\\ell 1} + \\cdots + w_{\\ell \\ell}\\end{pmatrix} = 0$$\n",
    "\n",
    "Положим, что существует соответствующий нулевому собственному значению собственный вектор $f' \\in \\mathbb{R}^n: $ $ \\exists p \\neq q $ такие, что $f'_p \\neq f'_q$ - т.е. он не является константным. Тогда $Lf' = 0 \\Rightarrow f'^T L f' = 0$. Поскольку мы рассматриваем связный граф, существует путь $p = i_0 \\to i_1 \\cdots \\to i_{n-1} \\to i_{n} = q$. Соседние вершины соединены ребром - отсюда следует, что $w_{i_j i_{j+1}} > 0$ - отсюда следует, что $f_{i_j} = f_{i_{j+1}}$ - иначе $f'^T L f > 0$. Отсюда же по цепочке выводится, что $f'_p = f'_q$ - противоречие. Значит $f'$ не будет собственным вектором для нулевого собственного значения, и мы доказали оба пункта.\n",
    "\n",
    "Теперь пусть $k>1$. Перенумеруем вершины в графе так, чтобы матрица $L$ приняла блочно-диагональный вид: \n",
    "\n",
    "$$L = \\begin{pmatrix} L_1 & 0 & \\cdots & 0 \\\\ 0 & L_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & 0 \\\\ 0 & 0 & \\cdots & L_k \\end{pmatrix} $$\n",
    "\n",
    "Каждый блок является лапласианом соответствующей компоненты связности графа, значит, нулевое собственное значение будет иметь кратность $k$, а собственные векторы $f_1,...,f_k$ будут задаваться по формуле из условия. Q.E.D.\n",
    "\n",
    "Кроме того, существует недоказанная, но проверенная эмпирически гипотеза. Она заключается в том, что для близких объектов $x_j, x_k$ (расстояние между ними мало) собственные векторы $f_i$, соответствующие малым собственным значениям, имеют близкие значения нужных координат: $f_{ij} \\approx f_{ik}$.\n",
    "\n",
    "Теперь мы можем сформулировать алгоритм спектральной кластеризации, основываясь на этой гипотезе:\n",
    "\n",
    "1. Строим по объектам граф $G$ и его лапласиан $L = D - W$.\n",
    "\n",
    "2. Находим нормированные собственные векторы $u_1,\\cdots,u_m$, соответствующие $m$ наименьшим собственным значениям.\n",
    "\n",
    "3. Составляем матрицу $U$, столбцы которой будут являться данными собственными векторами. По сути, мы переходим в новое признаковое пространство, где признаками являются собственные вектора.\n",
    "\n",
    "4. Обучаем на этой матрице K-Means с $k$ кластерами.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
