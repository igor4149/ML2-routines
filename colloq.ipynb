{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ядра. Теорема Мерсера. Способы построения ядер. Полиномиальные и гауссовы ядра.\n",
    "\n",
    "### здесь и далее важные на первом этапе вещи обрамляются конструкцией [Входит в теормин] - []\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Ядром называется функция $K(x,z)$, представимая в виде скалярного произведения в некотором пространстве: $K(x,z) = \\langle \\phi(x), \\phi(z) \\rangle$, где функция $\\phi: X \\rightarrow H$ является отображением из исходного пространства в некоторое спрямляющее пространство.\n",
    "\n",
    "Также можно определить ядро с помощью теоремы Мерсера: функция $K(x, z)$ является ядром $\\iff$\n",
    "\n",
    "1. $K(x, z) = K(z, x)$ - функция симметрична\n",
    "2. Для любой конечной выборки $(x_1,..., x_\\ell)$ матрица $K = \\left(K(x_i, x_j)\\right)_{i,j=1}^{\\ell}$ неотрицательно определена - функция неотрицательно определена\n",
    "\n",
    "**[]**\n",
    "\n",
    "Пользоваться этим на практике сложно, поэтому для построения ядер используются некоторые базовые вещи:\n",
    "\n",
    "Пусть нам даны ядра $K_1(x, z), K_2(x, z)$ на пространстве $X$, $f: X \\to \\mathbb{R}$ - вещественная функция на $X$, $\\phi: X \\to \\mathbb{R}^N$ - векторная функция на $X$ и $K_3$ - ядро на пространстве $\\mathbb{R}^N$. Тогда ядро можно получить с помощью следующих операций:\n",
    "\n",
    "1. $K(x, z) = K_1(x, z) + K_2(x, z)$\n",
    "\n",
    "2. $K(x, z) = \\alpha K_1(x, z), \\alpha > 0$\n",
    "\n",
    "3. $K(x,z) = K_1(x, z) \\cdot K_2(x, z)$\n",
    "\n",
    "4. $K(x,z) = f(x) \\cdot f(z)$\n",
    "\n",
    "5. $K(x,z) = K_3(\\phi(x), \\phi(z))$\n",
    "\n",
    "Пусть $K_1(x, z), K_2(x, z)...$ - последовательность ядер, причем $ K(x,z) = \\lim_{n \\to \\infty} K_n(x,z)$ $\\exists \\forall x, z$. Тогда $K(x,z)$ - ядро.\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Пусть $p(v)$ - многочлен с положительными коэффициентами. Очевидно, $p(K(x,z))$ является ядром для любого $K(x,z)$. \n",
    "\n",
    "Полиномиальное ядро степени $m$ можно определить как $K_m(x,z) = \\left(\\langle x,z \\rangle + R \\right)^m$. Согласно формуле бинома, \n",
    "\n",
    "$$K_m(x,z) = \\sum_{i=0}^m C_m^i R^{m-i} \\langle x,z \\rangle^i$$\n",
    "\n",
    "Все коэффициенты положительны, поэтому это действительно ядро. Оно соответствует переводу признаков во все возможные наборы мономов над признаками степени не более $m$.\n",
    "\n",
    "___________\n",
    "\n",
    "Гауссово ядро определяется как $K(x,z) = \\exp{\\left(-\\frac{||x-z||^2}{2\\sigma^2} \\right)}$. Докажем, что это действительно ядро:\n",
    "\n",
    "$$\\exp{(\\langle x,z \\rangle)} = \\sum_{k=0}^{\\infty} \\frac{\\langle x,z \\rangle^k}{k!} = \\lim_{n \\to \\infty} \\sum_{k=0}^{n} \\frac{\\langle x,z \\rangle^k}{k!}$$\n",
    "\n",
    "Каждый член этой числовой последовательности - ядро как многочлен с положительными коэффициентами, предел существует во всех точках (т.к. ряд Тейлора экспоненты сходится всюду) - отсюда получаем, что экспонента скалярного произведения является ядром.\n",
    "\n",
    "Аналогично доказывается ядерность $\\exp{\\left(\\frac{\\langle x,z \\rangle}{\\sigma^2}\\right)}$.\n",
    "\n",
    "Осталось перейти к исходному ядру, для этого сделаем так\n",
    "\n",
    "$$||x - z||^2 = \\langle x,x \\rangle + \\langle z,z \\rangle -2 \\langle x,z \\rangle$$\n",
    "\n",
    "$$\\exp{(-||x - z||^2)} = \\frac{\\exp(2\\langle x,z \\rangle)}{\\exp(||x||^2) \\exp(||z||^2)}$$\n",
    "\n",
    "Сверху ядро, снизу произведение вещественных функций - ядро, общее произведение - ядро.\n",
    "\n",
    "Это ядро задает бесконечномерное полиномиальное пространство (доказывается Тейлором).\n",
    "\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Задача условной оптимизации. Двойственная задача. Теорема Куна-Таккера.\n",
    "\n",
    "Запишем некоторую задачу оптимизации с ограничениями:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^d} \\\\\n",
    "\\\\\n",
    "f_i(x) \\leq 0, & \\forall i \\in \\{1,...,m\\} \\\\\n",
    "\\\\\n",
    "h_i(x) = 0, & \\forall i \\in \\{1,...,p\\} \\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Собственно, это и есть задача условной оптимизации.\n",
    "\n",
    "Из нее можно получить задачу безусловной оптимизации:\n",
    "\n",
    "$$f_0(x) + \\sum\\limits_{i=1}^{m} I\\_(f_i(x)) + \\sum\\limits_{i=1}^{p} I_0(h_i(x)) \\to \\min\\limits_x$$\n",
    "\n",
    "$$I\\_(x) = \\begin{align}\\begin{cases} 0, & x \\leq 0 \\\\ \\infty, & x > 0 \\end{cases}\\end{align}$$\n",
    "\n",
    "$$I_0(x) = \\begin{align}\\begin{cases} 0, & x = 0 \\\\ \\infty, & x \\neq 0 \\end{cases}\\end{align}$$\n",
    "\n",
    "Но это сложно оптимизировать, поэтому заменим индикаторы на линейные аппроксимации:\n",
    "\n",
    "$$L(x, \\lambda, \\mu) = f_0(x) + \\sum\\limits_{i=1}^{m} \\lambda_i f_i(x) + \\sum\\limits_{i=1}^{p} \\mu_i h_i(x)$$\n",
    "\n",
    "Выражение выше называется лагранжианом исходной задачи, а новые коэффициенты $\\lambda, \\mu$ - двойственными переменными.\n",
    "\n",
    "Теперь мы можем выразить через него двойственную функцию исходной задачи:\n",
    "\n",
    "$$g(\\lambda, \\mu) = \\inf\\limits_x L(x, \\lambda, \\mu)$$\n",
    "\n",
    "Эта функция дает нижнюю оценку на минимум в исходной задаче.\n",
    "\n",
    "$$g(\\lambda, \\mu) \\leq f_0(x_*)$$\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Соответственно, двойственную к исходной задачу можно представить как максимизацию двойственной функции с ограничениями на новые коэффициенты:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "g(\\lambda, \\mu) \\to \\max\\limits_{\\lambda, \\mu} \\\\\n",
    "\\lambda_i \\geq 0,& i \\in \\{1,...,m\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "**[]**\n",
    "\n",
    "Очевидно, для любого решения двойственной задачи $(\\lambda^*, \\mu^*)$ значение двойственной функции не превосходит условный минимум исходной задачи: $$g(\\lambda^*, \\mu^*) \\leq f_0(x^*)$$\n",
    "\n",
    "Это свойство называется слабой двойственностью, в случае равенства имеет место сильная двойственность.\n",
    "\n",
    "Пусть у нас есть решения прямой и двойственной задач $x_*$ и $(\\lambda^*, \\mu^*)$ соответственно, и пусть имеет место сильная двойственность. В таком случае $$f_0(x_*) = g(\\lambda^*, \\mu^*) = \\inf\\limits_{x} \\left( f_0(x) + \\sum\\limits_{i=1}^{m} \\lambda^*_i f_i(x) + \\sum\\limits_{i=1}^p \\mu^*_i h_i(x) \\right) \\leq $$ $$ \\leq f_0(x_*) + \\sum\\limits_{i=1}^{m} \\lambda^*_i f_i(x_*) + \\sum\\limits_{i=1}^p \\mu^*_i h_i(x_*) \\leq f_0(x_*)$$\n",
    "\n",
    "Все неравенства здесь превращаются в равенства. Отсюда мы можем получить, что $\\lambda_i^* f_i(x_*) = 0$ $\\forall i \\in \\{1,...,m\\}$ (следует из равенства суммы нулю и неположительности каждого слагаемого). Эти условия называются условиями дополняющей нежесткости, согласно им множитель $\\lambda$ при $i-$ом ограничении может быть ненулевым, только если оно выполняется с равенством (является активным). \n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Запишем условия, выполненные для решения прямой и двойственной задач:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\nabla f_0(x_*) + \\sum\\limits_{i=1}^m \\lambda_i^* \\nabla f_i(x_*) + \\sum\\limits_{i=1}^p \\mu_i^* \\nabla h_i(x_*) = 0 \\\\\n",
    "f_i(x_*) \\leq 0,& i \\in \\{1,...,m\\}\\\\\n",
    "h_i(x_*) = 0,& i \\in \\{1,...,p\\}\\\\\n",
    "\\lambda_i^* \\geq 0,& i \\in \\{1,...,m\\}\\\\\n",
    "\\lambda_i^* f_i(x_*) = 0,& i \\in \\{1,...,m\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Эти условия называются условиями Каруша-Куна-Таккера и являются необходимыми условиями экстремума. Можно сформулировать в виде теоремы: пусть есть решение $x_*$, тогда найдутся такие $\\lambda^*, \\mu^*$, что выполнены условия ККТ.\n",
    "\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Двойственная задача SVM (с выводом). Какие объекты называются периферийными, опорными граничными, опорными нарушителями?\n",
    "\n",
    "Исходная задача SVM формулируется так:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{1}{2}||w||^2 + C \\sum\\limits_{i=1}^\\ell \\xi_i \\to \\min\\limits_{w,b,\\xi}\\\\\n",
    "y_i(\\langle w, x_i \\rangle + b) \\geq 1 - \\xi_i,& i \\in \\{1,...,\\ell\\}\\\\\n",
    "\\xi_i \\geq 0,& i \\in \\{1,...,\\ell\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Запишем ее лагранжиан:\n",
    "\n",
    "$$L(w,b,\\xi,\\lambda,\\mu) = \\frac{1}{2}||w||^2 + C \\sum\\limits_{i=1}^\\ell \\xi_i - \\sum\\limits_{i=1}^\\ell \\lambda_i \\left[y_i (\\langle w, x_i \\rangle + b) - 1 + \\xi_i \\right] - \\sum\\limits_{i=1}^\\ell \\mu_i \\xi_i$$\n",
    "\n",
    "Сформулируем условия Куна-Таккера:\n",
    "\n",
    "1. $\\nabla_w L = w - \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i = 0 \\Longrightarrow w = \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i$\n",
    "\n",
    "\n",
    "2. $\\nabla_b L = - \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i = 0 \\Longrightarrow \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i = 0$ \n",
    "\n",
    "\n",
    "3. $\\nabla_{\\xi_i} L = C - \\lambda_i - \\mu_i = 0 \\Longrightarrow \\lambda_i + \\mu_i = C$\n",
    "\n",
    "\n",
    "4. $\\lambda_i \\left[y_i (\\langle w, x_i \\rangle + b) - 1 + \\xi_i \\right] = 0\\Longrightarrow \n",
    "(\\lambda_i = 0)$ или $ (y_i (\\langle w, x_i \\rangle + b) = 1 - \\xi_i) $\n",
    "\n",
    "\n",
    "5. $\\mu_i \\xi_i = 0 \\Longrightarrow (\\mu_i = 0)$ или $(\\xi_i = 0)$\n",
    "\n",
    "\n",
    "6. $\\lambda_i \\geq 0, \\mu_i \\geq 0, \\xi_i \\geq 0$\n",
    "\n",
    "Здесь можно выделить три случая на основании значений $\\xi, \\lambda$:\n",
    "\n",
    "1. $\\xi_i = 0, \\lambda_i = 0$ - в таком случае объект не участвует в формировании вектора весов (входит в сумму с нулевым весом) и при этом верно классифицируется (штраф 0). Назовем его периферийным.\n",
    "\n",
    "\n",
    "2. $\\xi_i = 0, 0 < \\lambda_i \\leq C$ - в таком случае объект верно классифицируется (штраф 0), при этом он находится на границе разделяющей полосы (отступ ровно 1) и дает ненулевой вклад в вектор весов. Назовем такой объект опорным граничным.\n",
    "\n",
    "\n",
    "3. $\\xi_i > 0, \\lambda_i = C$ - в таком случае объект лежит либо внутри разделяющей полосы ($0 < \\xi_i < 2$, при этом при $\\xi_i < 1$ он классифицируется верно, иначе неверно), либо выходит за ее пределы (и классифицируется неверно) и дает ненулевой вклад в вектор весов. Такой объект будем называть опорным нарушителем. \n",
    "\n",
    "Заметим, что эти случаи покрывают все возможные значения $\\lambda, \\xi$ - к примеру, случая $\\xi_i > 0, \\lambda_i < C$ быть не может в силу условий 5 и 3.\n",
    "\n",
    "Теперь сформулируем двойственную задачу. Сначала нам надо написать двойственную функцию (подставим условия 1-3 в лагранжиан):\n",
    "\n",
    "$$L = \\frac{1}{2} \\left|\\left|\\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i\\right|\\right|^2 - \\sum\\limits_{i,j=1}^{\\ell} \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle - b \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i + \\sum\\limits_{i=1}^{\\ell} \\lambda_i + \\sum_{i=1}^{\\ell} \\xi_i (C - \\lambda_i - \\mu_i) =$$ $$ = \\sum_{i=1}^\\ell \\lambda_i - \\frac{1}{2} \\sum\\limits_{i=1}^\\ell \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle$$\n",
    "\n",
    "Эту функцию нужно будет максимизировать. Ограничения мы введем на неотрицательность двойственных переменных $\\lambda, \\mu$, а также потребуем выполнение условий 2 и 3. Получаем такую систему: \n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\sum_{i=1}^\\ell \\lambda_i - \\frac{1}{2} \\sum\\limits_{i=1}^\\ell \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle \\to \\max\\limits_\\lambda \\\\\n",
    "\\\\\n",
    "0 \\leq \\lambda_i \\leq C,& i \\in \\{1,..., \\ell\\} \\\\\n",
    "\\\\\n",
    "\\sum\\limits_{i=1}^\\ell \\lambda_i y_i = 0\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Аппроксимация спрямляющего пространства: метод случайных признаков Фурье.\n",
    "\n",
    "Чтобы избавиться от довольно затратного хранения матрицы Грама для признакового описания выборки, попробуем в явном виде построить преобразование $\\tilde{\\phi}(x)$ пространства в пространство небольшой размерности, в котором уже можно обучать модели напрямую. Для этого используем метод случайных признаков Фурье (Random Kitchen Sinks). Его особенность в том, что он может аппроксимировать скалярное произведение, т.е. $\\langle \\tilde{\\phi}(x), \\tilde{\\phi}(z) \\rangle \\approx K(x,z)$\n",
    "\n",
    "Из комплексного анализа известно (кому это блять известно), что любое непрерывное ядро вида $K(x,z) = K(x-z)$ является преобразованием Фурье некоторого вероятностного распределения: $$K(x-z) = \\int_{\\mathbb{R}^d} p(w) e^{iw^T(x-z)} dw = \\int_{\\mathbb{R}^d} p(w) \\cos\\left(w^T (x-z)\\right)dw + i\\int_{\\mathbb{R}^d} p(w) \\sin\\left(w^T (x-z)\\right)dw$$\n",
    "\n",
    "Поскольку значение ядра всегда вещественное, мнимую часть интеграла можно приравнять к нулю. Вещественную часть (с косинусом) приблизим через Монте-Карло:\n",
    "\n",
    "$$\\int_{\\mathbb{R}^d} p(w) \\cos\\left(w^T (x-z)\\right)dw \\approx \\frac{1}{n} \\sum\\limits_{j=1}^{n} \\cos\\left(w_j^T(x-z)\\right)$$\n",
    "\n",
    "Здесь векторы $w_1,...,w_n$ сэмплируются из вероятностного распределения $p(w)$. Используя эти векторы, можем аппроксимировать искомое преобразование: \n",
    "\n",
    "$$\\phi(x) \\approx \\tilde{\\phi}(x) =  \\frac{1}{\\sqrt{n}} \\left(\\cos(w_1^Tx),...,\\cos(w_n^Tx),\\sin(w_1^T(x),...,\\sin(w_n^Tx)\\right)$$\n",
    "\n",
    "В таком случае скалярное произведение новых признаков будет иметь вид $$\\tilde{K}(x,z) = \\langle \\tilde{\\phi}(x), \\tilde{\\phi}(z) \\rangle = \\frac{1}{n} \\sum\\limits_{j=1}^n \\left(\\cos(w_j^Tx)\\cos(w_j^Tz) + \\sin(w_j^Tx)\\sin(w_j^Tz) \\right) = $$ $$ = \\frac{1}{n} \\sum\\limits_{j=1}^n \\cos\\left(w_j^T(x-z)\\right)$$\n",
    "\n",
    "Данная аппроксимация является несмещенной оценкой для $K(x,z)$. Чаще всего данный метод используется для Гауссовых ядер, поскольку для них распределение $p(w)$ довольно просто найти - оно будет нормальным со средним 0 и дисперсией $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Скрытые переменные, полное и неполное правдоподобие на примере смеси распределений. EM-алгоритм (описание шагов).\n",
    "\n",
    "Распределение $p(x)$ назовем смесью распределений, если его плотность имеет вид $$p(x) = \\sum\\limits_{k=1}^K p_k(x) \\pi_k(x), $$ $$\\sum\\limits_{k=1}^K \\pi_k = 1, \\pi_k \\geq 0,$$\n",
    "\n",
    "где $p_k(x)$ - параметризованные распределения компонент смеси $\\left(p_k(x) = \\phi(x|\\theta_k)\\right)$, $\\pi_k$ - априорные вероятности этих компонент, $K$ - их число. \n",
    "\n",
    "(не знаю, нужно ли это рассказывать, но в целом далее идет тривиальное рукомахание)\n",
    "\n",
    "Предположим, мы сэмплируем $x$ следующим образом - сначала из дискретного распределения $\\{\\pi_1,...,\\pi_k\\}$ выбирается номер компоненты $k$, а затем из распределения с плотностью $\\phi(x|\\theta_k)$ сэмплируется $x$. В таком случае распределение $x$ описывается вышеупомянутой смесью. \n",
    "\n",
    "Чтобы показать это, добавим в модель скрытую переменную $z$, которая будет являться $K$-мерным случайным one-hot вектором, отвечающим за выбор компоненты в смеси. При этом вектор сэмплируется из априорного распределения $\\pi$, т.е. $p(z_k = 1) = \\pi_k$. В таком случае распределение всего вектора выглядит так: $$p(z) = \\prod\\limits_{k=1}^k \\pi_k^{z_k}$$ \n",
    "\n",
    "Если мы уже знаем номер компоненты, то можем легко описать распределение $x$:\n",
    "\n",
    "$$p(x|z_k = 1) = \\phi(x|\\theta_k)$$ \n",
    "\n",
    "И обобщить это следующим образом:\n",
    "\n",
    "$$p(x|z) = \\prod\\limits_{k=1}^K \\left[\\phi(x|\\theta_k)\\right]^{z_k}$$ \n",
    "\n",
    "Отсюда выражается совместное распределение $x$ и $z$:\n",
    "\n",
    "$$p(x,z) = p(z)p(x|z) = \\prod\\limits_{k=1}^K \\left[\\pi_k \\phi(x|\\theta_k)\\right]^{z_k}$$ \n",
    "\n",
    "Чтобы найти распределение $x$, нужно маргинализировать совместное распределение по $z$, т.е. просуммировать по всем возможным one-hot векторам:\n",
    "\n",
    "$$p(x) = \\sum_z p(x,z) = \\sum_z \\left(\\prod\\limits_{k=1}^K \\left[\\pi_k \\phi(x|\\theta_k)\\right]^{z_k}\\right) = \\sum_{k=1}^K \\pi_k \\phi(x|\\theta_k)$$ \n",
    "\n",
    "(теперь о серьезных вещах)\n",
    "\n",
    "Допустим, у нас есть вероятностная модель с наблюдаемыми переменными $X$ и параметрами $\\Theta$, для которой задана функция лог-правдоподобия $\\log p(X|\\Theta)$. Допустим также, что в этой модели имеются скрытые переменные $Z$, которые описывают ее внутреннее состояние. Тогда будем называть правдоподобие наблюдаемых переменных $\\log p(X|\\Theta)$ неполным, а совместное правдоподобие наблюдаемых и скрытых переменных $\\log p(X,Z|\\Theta)$ - полным. Неполное правдоподобие можно получить из полного маргинализацией по скрытым переменным: $$\\log p(X|\\Theta) = \\log{\\left[ \\sum_Z p(X,Z|\\Theta)\\right]}$$ \n",
    "\n",
    "Рассмотрим эту модель для смеси распределений. Наблюдаемые переменные $X$ - это наша выборка $\\{x_1,...,x_\\ell \\}$. Скрытые переменные $Z$ - номера компонент $\\{z_1,...,z_\\ell\\}$ (каждый представлен one-hot вектором), из которых сэмплированы объекты выборки. Параметрами $\\Theta = (\\theta_1,...,\\theta_K,\\pi_1,...,\\pi_k)$ же будут являться априорные вероятности выбора компонент и параметры распределения каждой компоненты.\n",
    "\n",
    "**[Входит в теормин]**\n",
    "Неполное правдоподобие этой модели выражается так:\n",
    "\n",
    "$$\\log p(X|\\Theta) = \\sum\\limits_{i=1}^\\ell \\log{\\left[ \\sum\\limits_{k=1}^K \\pi_k \\phi(x_i | \\theta_k) \\right]}$$ \n",
    "\n",
    "Оно крайне неудобно для оптимизации, поскольку является логарифмом суммы. Поэтому перейдем к полному правдоподобию:\n",
    "\n",
    "$$\\log p(X,Z|\\Theta) = \\sum\\limits_{i=1}^\\ell \\sum\\limits_{k=1}^K z_{ik} \\left[\\log \\pi_k + \\log \\phi(x_i | \\theta_k) \\right]$$ \n",
    "**[]**\n",
    "\n",
    "Это уже удобнее оптимизировать, но есть такая проблема, что для нахождения ОМП на $\\Theta$ нам должны быть известны скрытые переменные $Z$. Они нам неизвестны, поэтому придется оценивать их одновременно с параметрами. Отсюда и вытекает ЕМ-алгоритм.\n",
    "\n",
    "В сущности, мы хотим максимизировать полное правдоподобие, попеременно находя наиболее вероятные значения для $Z$ и для $\\Theta$. Получается, что мы будем чередовать следующие шаги: \n",
    "\n",
    "$$ Z^* = \\arg\\max_Z p(X, Z|\\Theta^{old}) $$ \n",
    "\n",
    "$$\\Theta^{new} = \\arg\\max_\\Theta p(X, Z^* | \\Theta)$$ \n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Теперь, сформулируем более четко. \n",
    "\n",
    "На Е-шаге (Expectation) мы выбираем наиболее вероятное распределение скрытых переменных $Z$ - используем байесовский подход и вычислим здесь апостериорное распределение при фиксированных параметрах $p(Z|X,\\Theta^{old})$.\n",
    "\n",
    "На М-шаге (Maximization) мы подбираем такие параметры $\\Theta$, которые максимизируют ожидание полного лог-правдоподобия по апостериорному распределению $Z$:\n",
    "\n",
    "$$Q(\\Theta, \\Theta^{old}) = \\mathbb{E}_{Z \\sim p(Z|X,\\Theta^{old})} \\log p(X,Z|\\Theta) = \\sum_Z p(Z|X, \\Theta^{old}) \\log p(X,Z|\\Theta)$$ \n",
    "\n",
    "$$\\Theta^{new} = \\arg\\max_\\Theta Q(\\Theta, \\Theta^{old}) = \\arg\\max_\\Theta \\sum_Z p(Z|X, \\Theta^{old}) \\log p(X,Z|\\Theta)$$\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Дивергенция Кульбака-Лейблера, её неотрицательность. Вывод E- и M-шагов через разложение логарифма неполного правдоподобия на нижнюю оценку и KL-дивергенцию.\n",
    "\n",
    "WIP\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
