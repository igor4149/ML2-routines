{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ядра. Теорема Мерсера. Способы построения ядер. Полиномиальные и гауссовы ядра.\n",
    "\n",
    "### здесь и далее важные на первом этапе вещи обрамляются конструкцией [Входит в теормин] - []\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Ядром называется функция $K(x,z)$, представимая в виде скалярного произведения в некотором пространстве: $K(x,z) = \\langle \\phi(x), \\phi(z) \\rangle$, где функция $\\phi: X \\rightarrow H$ является отображением из исходного пространства в некоторое спрямляющее пространство.\n",
    "\n",
    "Также можно определить ядро с помощью теоремы Мерсера: функция $K(x, z)$ является ядром $\\iff$\n",
    "\n",
    "1. $K(x, z) = K(z, x)$ - функция симметрична\n",
    "2. Для любой конечной выборки $(x_1,..., x_\\ell)$ матрица $K = \\left(K(x_i, x_j)\\right)_{i,j=1}^{\\ell}$ неотрицательно определена - функция неотрицательно определена\n",
    "\n",
    "**[]**\n",
    "\n",
    "Пользоваться этим на практике сложно, поэтому для построения ядер используются некоторые базовые вещи:\n",
    "\n",
    "Пусть нам даны ядра $K_1(x, z), K_2(x, z)$ на пространстве $X$, $f: X \\to \\mathbb{R}$ - вещественная функция на $X$, $\\phi: X \\to \\mathbb{R}^N$ - векторная функция на $X$ и $K_3$ - ядро на пространстве $\\mathbb{R}^N$. Тогда ядро можно получить с помощью следующих операций:\n",
    "\n",
    "1. $K(x, z) = K_1(x, z) + K_2(x, z)$\n",
    "\n",
    "2. $K(x, z) = \\alpha K_1(x, z), \\alpha > 0$\n",
    "\n",
    "3. $K(x,z) = K_1(x, z) \\cdot K_2(x, z)$\n",
    "\n",
    "4. $K(x,z) = f(x) \\cdot f(z)$\n",
    "\n",
    "5. $K(x,z) = K_3(\\phi(x), \\phi(z))$\n",
    "\n",
    "Пусть $K_1(x, z), K_2(x, z)...$ - последовательность ядер, причем $ K(x,z) = \\lim_{n \\to \\infty} K_n(x,z)$ $\\exists \\forall x, z$. Тогда $K(x,z)$ - ядро.\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Пусть $p(v)$ - многочлен с положительными коэффициентами. Очевидно, $p(K(x,z))$ является ядром для любого $K(x,z)$. \n",
    "\n",
    "Полиномиальное ядро степени $m$ можно определить как $K_m(x,z) = \\left(\\langle x,z \\rangle + R \\right)^m$. Согласно формуле бинома, \n",
    "\n",
    "$$K_m(x,z) = \\sum_{i=0}^m C_m^i R^{m-i} \\langle x,z \\rangle^i$$\n",
    "\n",
    "Все коэффициенты положительны, поэтому это действительно ядро. Оно соответствует переводу признаков во все возможные наборы мономов над признаками степени не более $m$.\n",
    "\n",
    "___________\n",
    "\n",
    "Гауссово ядро определяется как $K(x,z) = \\exp{\\left(-\\frac{||x-z||^2}{2\\sigma^2} \\right)}$. Докажем, что это действительно ядро:\n",
    "\n",
    "$$\\exp{(\\langle x,z \\rangle)} = \\sum_{k=0}^{\\infty} \\frac{\\langle x,z \\rangle^k}{k!} = \\lim_{n \\to \\infty} \\sum_{k=0}^{n} \\frac{\\langle x,z \\rangle^k}{k!}$$\n",
    "\n",
    "Каждый член этой числовой последовательности - ядро как многочлен с положительными коэффициентами, предел существует во всех точках (т.к. ряд Тейлора экспоненты сходится всюду) - отсюда получаем, что экспонента скалярного произведения является ядром.\n",
    "\n",
    "Аналогично доказывается ядерность $\\exp{\\left(\\frac{\\langle x,z \\rangle}{\\sigma^2}\\right)}$.\n",
    "\n",
    "Осталось перейти к исходному ядру, для этого сделаем так\n",
    "\n",
    "$$||x - z||^2 = \\langle x,x \\rangle + \\langle z,z \\rangle -2 \\langle x,z \\rangle$$\n",
    "\n",
    "$$\\exp{(-||x - z||^2)} = \\frac{\\exp(2\\langle x,z \\rangle)}{\\exp(||x||^2) \\exp(||z||^2)}$$\n",
    "\n",
    "Сверху ядро, снизу произведение вещественных функций - ядро, общее произведение - ядро.\n",
    "\n",
    "Это ядро задает бесконечномерное полиномиальное пространство (доказывается Тейлором).\n",
    "\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Задача условной оптимизации. Двойственная задача. Теорема Куна-Таккера.\n",
    "\n",
    "Запишем некоторую задачу оптимизации с ограничениями:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^d} \\\\\n",
    "\\\\\n",
    "f_i(x) \\leq 0, & \\forall i \\in \\{1,...,m\\} \\\\\n",
    "\\\\\n",
    "h_i(x) = 0, & \\forall i \\in \\{1,...,p\\} \\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Собственно, это и есть задача условной оптимизации.\n",
    "\n",
    "Из нее можно получить задачу безусловной оптимизации:\n",
    "\n",
    "$$f_0(x) + \\sum\\limits_{i=1}^{m} I\\_(f_i(x)) + \\sum\\limits_{i=1}^{p} I_0(h_i(x)) \\to \\min\\limits_x$$\n",
    "\n",
    "$$I\\_(x) = \\begin{align}\\begin{cases} 0, & x \\leq 0 \\\\ \\infty, & x > 0 \\end{cases}\\end{align}$$\n",
    "\n",
    "$$I_0(x) = \\begin{align}\\begin{cases} 0, & x = 0 \\\\ \\infty, & x \\neq 0 \\end{cases}\\end{align}$$\n",
    "\n",
    "Но это сложно оптимизировать, поэтому заменим индикаторы на линейные аппроксимации:\n",
    "\n",
    "$$L(x, \\lambda, \\mu) = f_0(x) + \\sum\\limits_{i=1}^{m} \\lambda_i f_i(x) + \\sum\\limits_{i=1}^{p} \\mu_i h_i(x)$$\n",
    "\n",
    "Выражение выше называется лагранжианом исходной задачи, а новые коэффициенты $\\lambda, \\mu$ - двойственными переменными.\n",
    "\n",
    "Теперь мы можем выразить через него двойственную функцию исходной задачи:\n",
    "\n",
    "$$g(\\lambda, \\mu) = \\inf\\limits_x L(x, \\lambda, \\mu)$$\n",
    "\n",
    "Эта функция дает нижнюю оценку на минимум в исходной задаче.\n",
    "\n",
    "$$g(\\lambda, \\mu) \\leq f_0(x_*)$$\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Соответственно, двойственную к исходной задачу можно представить как максимизацию двойственной функции с ограничениями на новые коэффициенты:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "g(\\lambda, \\mu) \\to \\max\\limits_{\\lambda, \\mu} \\\\\n",
    "\\lambda_i \\geq 0,& i \\in \\{1,...,m\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "**[]**\n",
    "\n",
    "Очевидно, для любого решения двойственной задачи $(\\lambda^*, \\mu^*)$ значение двойственной функции не превосходит условный минимум исходной задачи: $$g(\\lambda^*, \\mu^*) \\leq f_0(x^*)$$\n",
    "\n",
    "Это свойство называется слабой двойственностью, в случае равенства имеет место сильная двойственность. Разность между правой и левой частью называется двойственным зазором.\n",
    "\n",
    "Для сильной двойственности в выпуклых задачах имеются различные достаточные условия. Сформулируем одно из них.\n",
    "\n",
    "Задача оптимизации называется выпуклой, если она имеет следующий вид:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "f_0(x) \\to \\min\\limits_{x \\in \\mathbb{R}^d}\\\\\n",
    "f_i(x) \\leq 0,& i \\in \\{1,...,m\\} \\\\\n",
    "Ax = b \\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "где функции $f_0, f_1,..., f_m$ являются выпуклыми. Условие Слейтера требует существование такой точки $x'$, в которой все ограничения бы выполнялись, при этом ограничения-неравенства были бы выполнены строго. Впрочем, достаточно, чтобы эти неравенства выполнялись строго, только если они имеют нелинейный вид.\n",
    "\n",
    "Пусть у нас есть решения прямой и двойственной задач $x_*$ и $(\\lambda^*, \\mu^*)$ соответственно, и пусть имеет место сильная двойственность. В таком случае $$f_0(x_*) = g(\\lambda^*, \\mu^*) = \\inf\\limits_{x} \\left( f_0(x) + \\sum\\limits_{i=1}^{m} \\lambda^*_i f_i(x) + \\sum\\limits_{i=1}^p \\mu^*_i h_i(x) \\right) \\leq $$ $$ \\leq f_0(x_*) + \\sum\\limits_{i=1}^{m} \\lambda^*_i f_i(x_*) + \\sum\\limits_{i=1}^p \\mu^*_i h_i(x_*) \\leq f_0(x_*)$$\n",
    "\n",
    "Все неравенства здесь превращаются в равенства. Отсюда мы можем получить, что $\\lambda_i^* f_i(x_*) = 0$ $\\forall i \\in \\{1,...,m\\}$ (следует из равенства суммы нулю и неположительности каждого слагаемого). Эти условия называются условиями дополняющей нежесткости, согласно им множитель $\\lambda$ при $i-$ом ограничении может быть ненулевым, только если оно выполняется с равенством (является активным). \n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Запишем условия, выполненные для решения прямой и двойственной задач:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\nabla f_0(x_*) + \\sum\\limits_{i=1}^m \\lambda_i^* \\nabla f_i(x_*) + \\sum\\limits_{i=1}^p \\mu_i^* \\nabla h_i(x_*) = 0 \\\\\n",
    "f_i(x_*) \\leq 0,& i \\in \\{1,...,m\\}\\\\\n",
    "h_i(x_*) = 0,& i \\in \\{1,...,p\\}\\\\\n",
    "\\lambda_i^* \\geq 0,& i \\in \\{1,...,m\\}\\\\\n",
    "\\lambda_i^* f_i(x_*) = 0,& i \\in \\{1,...,m\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Эти условия называются условиями Каруша-Куна-Таккера и являются необходимыми условиями экстремума. Можно сформулировать в виде теоремы: пусть есть решение $x_*$, тогда найдутся такие $\\lambda^*, \\mu^*$, что выполнены условия ККТ. Если задача является выпуклой и удовлетворяет условиям Слейтера, то условия ККТ будут необходимыми и достаточными. \n",
    "\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Двойственная задача SVM (с выводом). Какие объекты называются периферийными, опорными граничными, опорными нарушителями?\n",
    "\n",
    "Исходная задача SVM формулируется так:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{1}{2}||w||^2 + C \\sum\\limits_{i=1}^\\ell \\xi_i \\to \\min\\limits_{w,b,\\xi}\\\\\n",
    "y_i(\\langle w, x_i \\rangle + b) \\geq 1 - \\xi_i,& i \\in \\{1,...,\\ell\\}\\\\\n",
    "\\xi_i \\geq 0,& i \\in \\{1,...,\\ell\\}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Запишем ее лагранжиан:\n",
    "\n",
    "$$L(w,b,\\xi,\\lambda,\\mu) = \\frac{1}{2}||w||^2 + C \\sum\\limits_{i=1}^\\ell \\xi_i - \\sum\\limits_{i=1}^\\ell \\lambda_i \\left[y_i (\\langle w, x_i \\rangle + b) - 1 + \\xi_i \\right] - \\sum\\limits_{i=1}^\\ell \\mu_i \\xi_i$$\n",
    "\n",
    "Сформулируем условия Куна-Таккера:\n",
    "\n",
    "1. $\\nabla_w L = w - \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i = 0 \\Longrightarrow w = \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i$\n",
    "\n",
    "2. $\\nabla_b L = - \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i = 0 \\Longrightarrow \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i = 0$ \n",
    "\n",
    "3. $\\nabla_{\\xi_i} L = C - \\lambda_i - \\mu_i = 0 \\Longrightarrow \\lambda_i + \\mu_i = C$\n",
    "\n",
    "4. $\\lambda_i \\left[y_i (\\langle w, x_i \\rangle + b) - 1 + \\xi_i \\right] = 0\\Longrightarrow \n",
    "(\\lambda_i = 0)$ или $ (y_i (\\langle w, x_i \\rangle + b) = 1 - \\xi_i) $\n",
    "\n",
    "5. $\\mu_i \\xi_i = 0 \\Longrightarrow (\\mu_i = 0)$ или $(\\xi_i = 0)$\n",
    "\n",
    "6. $\\lambda_i \\geq 0, \\mu_i \\geq 0, \\xi_i \\geq 0$\n",
    "\n",
    "Здесь можно выделить три случая на основании значений $\\xi, \\lambda$:\n",
    "\n",
    "1. $\\xi_i = 0, \\lambda_i = 0$ - в таком случае объект не участвует в формировании вектора весов (входит в сумму с нулевым весом) и при этом верно классифицируется (штраф 0). Назовем его периферийным.\n",
    "\n",
    "2. $\\xi_i = 0, 0 < \\lambda_i \\leq C$ - в таком случае объект верно классифицируется (штраф 0), при этом он находится на границе разделяющей полосы (отступ ровно 1) и дает ненулевой вклад в вектор весов. Назовем такой объект опорным граничным.\n",
    "\n",
    "3. $\\xi_i > 0, \\lambda_i = C$ - в таком случае объект лежит либо внутри разделяющей полосы ($0 < \\xi_i < 2$, при этом при $\\xi_i < 1$ он классифицируется верно, иначе неверно), либо выходит за ее пределы (и классифицируется неверно) и дает ненулевой вклад в вектор весов. Такой объект будем называть опорным нарушителем. \n",
    "\n",
    "Заметим, что эти случаи покрывают все возможные значения $\\lambda, \\xi$ - к примеру, случая $\\xi_i > 0, \\lambda_i < C$ быть не может в силу условий 5 и 3.\n",
    "\n",
    "Теперь сформулируем двойственную задачу. Сначала нам надо написать двойственную функцию (подставим условия 1-3 в лагранжиан):\n",
    "\n",
    "$$L = \\frac{1}{2} \\left|\\left|\\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i x_i\\right|\\right|^2 - \\sum\\limits_{i,j=1}^{\\ell} \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle - b \\sum\\limits_{i=1}^{\\ell} \\lambda_i y_i + \\sum\\limits_{i=1}^{\\ell} \\lambda_i + \\sum_{i=1}^{\\ell} \\xi_i (C - \\lambda_i - \\mu_i) =$$ $$ = \\sum_{i=1}^\\ell \\lambda_i - \\frac{1}{2} \\sum\\limits_{i=1}^\\ell \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle$$\n",
    "\n",
    "Эту функцию нужно будет максимизировать. Ограничения мы введем на неотрицательность двойственных переменных $\\lambda, \\mu$, а также потребуем выполнение условий 2 и 3. Получаем такую систему: \n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\sum_{i=1}^\\ell \\lambda_i - \\frac{1}{2} \\sum\\limits_{i=1}^\\ell \\lambda_i \\lambda_j y_i y_j \\langle x_i, x_j \\rangle \\to \\max\\limits_\\lambda \\\\\n",
    "\\\\\n",
    "0 \\leq \\lambda_i \\leq C,& i \\in \\{1,..., \\ell\\} \\\\\n",
    "\\\\\n",
    "\\sum\\limits_{i=1}^\\ell \\lambda_i y_i = 0\n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Аппроксимация спрямляющего пространства: метод случайных признаков Фурье.\n",
    "\n",
    "Чтобы избавиться от довольно затратного хранения матрицы Грама для признакового описания выборки, попробуем в явном виде построить преобразование $\\tilde{\\phi}(x)$ пространства в пространство небольшой размерности, в котором уже можно обучать модели напрямую. Для этого используем метод случайных признаков Фурье (Random Kitchen Sinks). Его особенность в том, что он может аппроксимировать скалярное произведение, т.е. $\\langle \\tilde{\\phi}(x), \\tilde{\\phi}(z) \\rangle \\approx K(x,z)$\n",
    "\n",
    "Из комплексного анализа известно (кому это блять известно), что любое непрерывное ядро вида $K(x,z) = K(x-z)$ является преобразованием Фурье некоторого вероятностного распределения: $$K(x-z) = \\int_{\\mathbb{R}^d} p(w) e^{iw^T(x-z)} dw = \\int_{\\mathbb{R}^d} p(w) \\cos\\left(w^T (x-z)\\right)dw + i\\int_{\\mathbb{R}^d} p(w) \\sin\\left(w^T (x-z)\\right)dw$$\n",
    "\n",
    "Поскольку значение ядра всегда вещественное, мнимую часть интеграла можно приравнять к нулю. Вещественную часть (с косинусом) приблизим через Монте-Карло:\n",
    "\n",
    "$$\\int_{\\mathbb{R}^d} p(w) \\cos\\left(w^T (x-z)\\right)dw \\approx \\frac{1}{n} \\sum\\limits_{j=1}^{n} \\cos\\left(w_j^T(x-z)\\right)$$\n",
    "\n",
    "Здесь векторы $w_1,...,w_n$ сэмплируются из вероятностного распределения $p(w)$. Используя эти векторы, можем аппроксимировать искомое преобразование: \n",
    "\n",
    "$$\\phi(x) \\approx \\tilde{\\phi}(x) =  \\frac{1}{\\sqrt{n}} \\left(\\cos(w_1^Tx),...,\\cos(w_n^Tx),\\sin(w_1^T(x),...,\\sin(w_n^Tx)\\right)$$\n",
    "\n",
    "В таком случае скалярное произведение новых признаков будет иметь вид $$\\tilde{K}(x,z) = \\langle \\tilde{\\phi}(x), \\tilde{\\phi}(z) \\rangle = \\frac{1}{n} \\sum\\limits_{j=1}^n \\left(\\cos(w_j^Tx)\\cos(w_j^Tz) + \\sin(w_j^Tx)\\sin(w_j^Tz) \\right) = $$ $$ = \\frac{1}{n} \\sum\\limits_{j=1}^n \\cos\\left(w_j^T(x-z)\\right)$$\n",
    "\n",
    "Данная аппроксимация является несмещенной оценкой для $K(x,z)$. Чаще всего данный метод используется для Гауссовых ядер, поскольку для них распределение $p(w)$ довольно просто найти - оно будет нормальным со средним 0 и дисперсией $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Скрытые переменные, полное и неполное правдоподобие на примере смеси распределений. EM-алгоритм (описание шагов).\n",
    "\n",
    "Распределение $p(x)$ назовем смесью распределений, если его плотность имеет вид $$p(x) = \\sum\\limits_{k=1}^K p_k(x) \\pi_k(x), $$ $$\\sum\\limits_{k=1}^K \\pi_k = 1, \\pi_k \\geq 0,$$\n",
    "\n",
    "где $p_k(x)$ - параметризованные распределения компонент смеси $\\left(p_k(x) = \\phi(x|\\theta_k)\\right)$, $\\pi_k$ - априорные вероятности этих компонент, $K$ - их число. \n",
    "\n",
    "(не знаю, нужно ли это рассказывать, но в целом далее идет тривиальное рукомахание)\n",
    "\n",
    "Предположим, мы сэмплируем $x$ следующим образом - сначала из дискретного распределения $\\{\\pi_1,...,\\pi_k\\}$ выбирается номер компоненты $k$, а затем из распределения с плотностью $\\phi(x|\\theta_k)$ сэмплируется $x$. В таком случае распределение $x$ описывается вышеупомянутой смесью. \n",
    "\n",
    "Чтобы показать это, добавим в модель скрытую переменную $z$, которая будет являться $K$-мерным случайным one-hot вектором, отвечающим за выбор компоненты в смеси. При этом вектор сэмплируется из априорного распределения $\\pi$, т.е. $p(z_k = 1) = \\pi_k$. В таком случае распределение всего вектора выглядит так: $$p(z) = \\prod\\limits_{k=1}^k \\pi_k^{z_k}$$ \n",
    "\n",
    "Если мы уже знаем номер компоненты, то можем легко описать распределение $x$:\n",
    "\n",
    "$$p(x|z_k = 1) = \\phi(x|\\theta_k)$$ \n",
    "\n",
    "И обобщить это следующим образом:\n",
    "\n",
    "$$p(x|z) = \\prod\\limits_{k=1}^K \\left[\\phi(x|\\theta_k)\\right]^{z_k}$$ \n",
    "\n",
    "Отсюда выражается совместное распределение $x$ и $z$:\n",
    "\n",
    "$$p(x,z) = p(z)p(x|z) = \\prod\\limits_{k=1}^K \\left[\\pi_k \\phi(x|\\theta_k)\\right]^{z_k}$$ \n",
    "\n",
    "Чтобы найти распределение $x$, нужно маргинализировать совместное распределение по $z$, т.е. просуммировать по всем возможным one-hot векторам:\n",
    "\n",
    "$$p(x) = \\sum_z p(x,z) = \\sum_z \\left(\\prod\\limits_{k=1}^K \\left[\\pi_k \\phi(x|\\theta_k)\\right]^{z_k}\\right) = \\sum_{k=1}^K \\pi_k \\phi(x|\\theta_k)$$ \n",
    "\n",
    "(теперь о серьезных вещах)\n",
    "\n",
    "Допустим, у нас есть вероятностная модель с наблюдаемыми переменными $X$ и параметрами $\\Theta$, для которой задана функция лог-правдоподобия $\\log p(X|\\Theta)$. Допустим также, что в этой модели имеются скрытые переменные $Z$, которые описывают ее внутреннее состояние. Тогда будем называть правдоподобие наблюдаемых переменных $\\log p(X|\\Theta)$ неполным, а совместное правдоподобие наблюдаемых и скрытых переменных $\\log p(X,Z|\\Theta)$ - полным. Неполное правдоподобие можно получить из полного маргинализацией по скрытым переменным: $$\\log p(X|\\Theta) = \\log{\\left[ \\sum_Z p(X,Z|\\Theta)\\right]}$$ \n",
    "\n",
    "Рассмотрим эту модель для смеси распределений. Наблюдаемые переменные $X$ - это наша выборка $\\{x_1,...,x_\\ell \\}$. Скрытые переменные $Z$ - номера компонент $\\{z_1,...,z_\\ell\\}$ (каждый представлен one-hot вектором), из которых сэмплированы объекты выборки. Параметрами $\\Theta = (\\theta_1,...,\\theta_K,\\pi_1,...,\\pi_k)$ же будут являться априорные вероятности выбора компонент и параметры распределения каждой компоненты.\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Неполное правдоподобие этой модели выражается так:\n",
    "\n",
    "$$\\log p(X|\\Theta) = \\sum\\limits_{i=1}^\\ell \\log{\\left[ \\sum\\limits_{k=1}^K \\pi_k \\phi(x_i | \\theta_k) \\right]}$$ \n",
    "\n",
    "Оно крайне неудобно для оптимизации, поскольку является логарифмом суммы. Поэтому перейдем к полному правдоподобию:\n",
    "\n",
    "$$\\log p(X,Z|\\Theta) = \\sum\\limits_{i=1}^\\ell \\sum\\limits_{k=1}^K z_{ik} \\left[\\log \\pi_k + \\log \\phi(x_i | \\theta_k) \\right]$$ \n",
    "**[]**\n",
    "\n",
    "Это уже удобнее оптимизировать, но есть такая проблема, что для нахождения ОМП на $\\Theta$ нам должны быть известны скрытые переменные $Z$. Они нам неизвестны, поэтому придется оценивать их одновременно с параметрами. Отсюда и вытекает ЕМ-алгоритм.\n",
    "\n",
    "В сущности, мы хотим максимизировать полное правдоподобие, попеременно находя наиболее вероятные значения для $Z$ и для $\\Theta$. Получается, что мы будем чередовать следующие шаги: \n",
    "\n",
    "$$ Z^* = \\arg\\max_Z p(X, Z|\\Theta^{old}) $$ \n",
    "\n",
    "$$\\Theta^{new} = \\arg\\max_\\Theta p(X, Z^* | \\Theta)$$ \n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Теперь, сформулируем более четко. \n",
    "\n",
    "На Е-шаге (Expectation) мы выбираем наиболее вероятное распределение скрытых переменных $Z$ - используем байесовский подход и вычислим здесь апостериорное распределение при фиксированных параметрах $p(Z|X,\\Theta^{old})$.\n",
    "\n",
    "На М-шаге (Maximization) мы подбираем такие параметры $\\Theta$, которые максимизируют ожидание полного лог-правдоподобия по апостериорному распределению $Z$:\n",
    "\n",
    "$$Q(\\Theta, \\Theta^{old}) = \\mathbb{E}_{Z \\sim p(Z|X,\\Theta^{old})} \\log p(X,Z|\\Theta) = \\sum_Z p(Z|X, \\Theta^{old}) \\log p(X,Z|\\Theta)$$ \n",
    "\n",
    "$$\\Theta^{new} = \\arg\\max_\\Theta Q(\\Theta, \\Theta^{old}) = \\arg\\max_\\Theta \\sum_Z p(Z|X, \\Theta^{old}) \\log p(X,Z|\\Theta)$$\n",
    "**[]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Дивергенция Кульбака-Лейблера, её неотрицательность. Вывод E- и M-шагов через разложение логарифма неполного правдоподобия на нижнюю оценку и KL-дивергенцию.\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "KL-дивергенция является мерой расстояния между двумя распределениями. Формулируется она так: \n",
    "\n",
    "$$\\text{KL}(q || p) = \\int_{\\mathbb{R}} q(x) \\log \\frac{q(x)}{p(x)} dx$$\n",
    "\n",
    "(здесь интеграл берется по всему пространству)\n",
    "\n",
    "В дискретном случае: \n",
    "\n",
    "$$\\text{KL}(q || p) = \\sum_{x} q(x) \\log \\frac{q(x)}{p(x)}$$\n",
    "**[]**\n",
    "\n",
    "При вычислении полагаем, что все неопределенности обращаются в 0. Эта мера определена только в том случае, если из $p(x) = 0$ следует $q(x) = 0$.\n",
    "\n",
    "Можно заметить, что данная мера является асимметричной и неотрицательной. Неотрицательность можно доказать, если использовать неравенство $\\log(x) \\leq x - 1$. В таком случае $$\\text{KL}(q || p) = -\\sum_{x} q(x) \\log \\frac{p(x)}{q(x)} \\geq - \\sum_{x} q(x) \\left( \\frac{p(x)}{q(x)} - 1\\right) = \\sum_x p(x) - \\sum_x q(x) = 1 - 1 = 0$$\n",
    "\n",
    "**[Входит в теормин, не входит в основную часть))]**\n",
    "\n",
    "Пусть нам дана выборка $X^\\ell$ и распределение $p(x|\\theta)$, параметры которого мы хотим настроить под данную выборку. Эмпирическим распределением называется дискретное распределение на объектах, которое присваивает равные вероятности $\\frac{1}{\\ell}$ всем объектам из обучающей выборки: $$\\hat{p}\\left(x | X^\\ell\\right) = \\sum\\limits_{i=1}^\\ell \\frac{1}{\\ell} [x = x_i]$$. \n",
    "\n",
    "Максимизация правдоподобия эквивалентна минимизации $\\text{KL}\\left(\\hat{p}\\left(x | X^\\ell\\right) || p(x|\\theta)\\right)$ - KL-дивергенции между эмпирическим и модельным распределением. Докажем.\n",
    "\n",
    "$$\\text{KL}\\left(\\hat{p}\\left(x | X^\\ell\\right) || p(x|\\theta)\\right) = \\sum\\limits_{i=1}^\\ell \\frac{1}{\\ell} \\log \\frac{\\frac{1}{\\ell}}{p(x_i|\\theta)} = $$ $$ = \\sum\\limits_{i=1}^\\ell \\frac{1}{\\ell} \\log \\frac{1}{\\ell} - \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell \\log p(x_i|\\theta) \\to \\min_\\theta$$\n",
    "\n",
    "При отбрасывании константных членов видим, что это равносильно $\\log p(x_i|\\theta) \\to \\max\\limits_\\theta$ - задаче максимизации правдоподобия.\n",
    "\n",
    "**[]**\n",
    "\n",
    "С помощью KL-дивергенции мы можем вывести шаги ЕМ-алгоритма. Для начала разложим неполное лог-правдоподобие на сумму двух функций:\n",
    "\n",
    "$$\\log p(X|\\Theta) = \\mathcal{L}(q, \\Theta) + \\text{KL}(q||p)$$\n",
    "\n",
    "$$\\mathcal{L}(q, \\Theta) = \\sum_Z q(Z) \\log \\frac{p(X,Z|\\Theta)}{q(Z)}$$\n",
    "\n",
    "$$\\text{KL}(q||p) = -\\sum_Z q(Z) \\log \\frac{p(Z|X, \\Theta)}{q(Z)}$$\n",
    "\n",
    "Здесь $q(Z)$ - произвольное распределение на скрытых переменных. \n",
    "\n",
    "Разложение корректно, так как \n",
    "\n",
    "$$\\sum_Z q(Z) \\log \\frac{p(X,Z|\\Theta)}{q(Z)} - \\sum_Z q(Z) \\log \\frac{p(Z|X, \\Theta)}{q(Z)} = $$\n",
    "\n",
    "$$ = \\sum_Z q(Z) \\log \\frac{p(X,Z|\\Theta)}{p(Z|X,\\Theta)} = \\sum_Z q(Z) \\log p(X|\\Theta) = $$ \n",
    "\n",
    "$$ = \\log p(X|\\theta) \\sum_Z q(Z) = \\log p(X|\\theta)$$\n",
    "\n",
    "В силу неотрицательности KL-дивергенции можем сказать, что $\\mathcal{L}(q, \\Theta)$ будет являться нижней оценкой для неполного лог-правдоподобия $\\log p(X|\\Theta)$. Можно сказать, что чем корректнее выбрано распределение $q$, тем точнее будет нижняя оценка. \n",
    "\n",
    "Будем по очереди максимизировать эту нижнюю оценку по $q$ и по $\\Theta$. \n",
    "\n",
    "Зафиксируем параметры $\\Theta^{old}$. Поскольку неполное лог-правдоподобие никак не зависит от $q$, его нижняя оценка будет максимальна в том случае, если KL-дивергенция будет минимальна, то есть $q(Z) = p(Z|X,\\Theta^{old})$. Собственно, это и есть E-шаг алгоритма.\n",
    "\n",
    "Зафиксируем теперь $q(Z)$ и найдем максимум нижней оценки по $\\Theta$. Нам нужно решить следующую задачу оптимизации:\n",
    "\n",
    "$$\\mathcal{L}(q, \\Theta) = \\sum_Z q(Z) \\log \\frac{p(X,Z|\\Theta)}{q(Z)} = \\sum_Z q(Z) \\log p(X,Z|\\Theta) - \\sum_Z q(Z) \\log q(Z) =$$\n",
    "\n",
    "$$ = \\sum_Z p(Z|X, \\Theta^{old}) \\log p(X,Z|\\Theta) + \\text{const}(\\Theta) = Q(\\Theta, \\Theta^{old}) + \\text{const}(\\Theta) \\to \\max_\\Theta$$\n",
    "\n",
    "Именно этим мы и занимаемся на М-шаге.\n",
    "\n",
    "Отметим, что такое разложение позволяет нам показать важное свойство ЕМ-алгоритма - на каждой итерации значение его правдоподобия как минимум не уменьшается. Действительно, \n",
    "\n",
    "$$\\log p(X, \\Theta^{new}) = \\mathcal{L} (q, \\Theta^{new}) + \\text{KL}(q||p) \\geq \\mathcal{L} (q, \\Theta^{new}) \\geq \\mathcal{L} (q, \\Theta^{old}) = \\log p(X, \\Theta^{old}),$$\n",
    "\n",
    "так как после Е-шага мы занулили KL-дивергенцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Лапласиан графа, его свойства. Связь кратности нулевого собственного значения с числом компонент связности (с доказательством). Алгоритм спектральной кластеризации. \n",
    "\n",
    "Допустим, нам дана выборка $\\{X_1,...,X_\\ell\\}$ и нам захотелось представить ее в виде неориентированного графа. В таком случае мы можем задать вершины графа как объекты этой выборки, а ребра подкостылить следующими эвристиками:\n",
    "\n",
    "1. Использовать полный граф, в котором веса ребер определяются некоторой формулой. E.g.\n",
    "\n",
    "$$w_{ij} = \\exp \\left(- \\frac{||x_i - x_j||^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "$\\sigma$ отвечает за то, насколько нам важны далекие объекты.\n",
    "\n",
    "2. Использовать k-NN граф. В нем каждая вершина будет соединена ребрами с $k$ ближайших соседей.\n",
    "\n",
    "3. Использовать $\\epsilon$-граф. В нем каждая вершина будет соединена с теми вершинами, расстояние до которых не превосходит некоторого $\\epsilon$. \n",
    "\n",
    "\n",
    "**[Входит в теормин]**\n",
    "\n",
    "Пусть нам дан граф $G$, имеющий матрицу смежности $W$. Степени вершин в таком графе будем обозначать как $d_i = \\sum_{j=1}^{n} w_{ij}$. Зададим матрицу $D = \\text{diag}(d_1,...,d_n)$ - диагональная матрица из степеней вершин. В таком случае лапласианом графа $G$ будет называться матрица $L = D - W$.\n",
    "\n",
    "Рассмотрим некоторые свойства лапласиана.\n",
    "\n",
    "1. Он симметричен. Поскольку мы работаем с вышеобозначенной задачей, в которой граф неориентирован, это действительно так.\n",
    "\n",
    "2. Пусть $f \\in \\mathbb{R}^n$. Тогда справедлива следующая формула: $$f^T L f = \\frac{1}{2} \\sum\\limits_{i,j=1}^{n} w_{ij} (f_i - f_j)^2$$ Док-во:\n",
    "\n",
    "$$f^T L f = f^T D f - f^T W f = \\sum\\limits_{i=1}^n d_i f_i^2 - \\sum\\limits_{i,j=1}^n w_{ij} f_i f_j = $$\n",
    "\n",
    "$$ = \\sum\\limits_{i=1}^n \\left(\\sum\\limits_{i,j=1}^{n} w_{ij} \\right) f_i^2 - \\sum\\limits_{i,j=1}^n w_{ij} f_i f_j = \\sum\\limits_{i,j=1}^n w_{ij} f_i^2 - \\sum\\limits_{i,j=1}^n w_{ij} f_i f_j = $$ \n",
    "\n",
    "$$ = \\sum\\limits_{i,j=1}^n w_{ij} (f_i^2 - f_i f_j) = [\\text{symmetric}] = \\sum\\limits_{i=1}^n \\sum\\limits_{j=i+1}^n w_{ij} (f_i^2 - 2 f_i f_j + f_j^2) = $$\n",
    "\n",
    "$$ = \\sum\\limits_{i=1}^n \\sum\\limits_{j=i+1}^n w_{ij} (f_i -  f_j)^2 = \\frac{1}{2} \\sum\\limits_{i,j=1}^n w_{ij} (f_i^2 - f_i f_j)$$\n",
    "\n",
    "3. Он неотрицательно определен. В самом деле, в формуле выше каждое слагаемое является произведением двух неотрицательных элементов, в таком случае $f^T L f \\geq 0$  $\\forall f$ - что и означает неотрицательную определенность.\n",
    "\n",
    "**[]**\n",
    "\n",
    "Кроме того, у лапласиана есть свойство, которое позволяет использовать его для задачи кластеризации. Пусть $L$ - лапласиан графа $G$. Тогда мы можем сказать следующее:\n",
    "\n",
    "1. Нулевое собственное значение лапласиана имеет кратность $k$, равную числу компонент связности в графе.\n",
    "\n",
    "2. Пусть $A_1,...,A_k$ - компоненты связности графа $G$. Тогда векторы $f_1,...,f_k$ вида $f_i = ([x_j \\in A_i])_{j=1}^{\\ell}$ - индикаторные векторы вхождения объектов в данную компоненту связности для каждой компоненты - будут являться собственными векторами для нулевого собственного значения.\n",
    "\n",
    "Докажем это.\n",
    "\n",
    "Сперва рассмотрим случай $k = 1$. Покажем, что для матрицы $L$ $\\lambda = 0$ будет собственным значением. Для этого нам нужно рассмотреть вектор $f = (1,...,1)$. \n",
    "\n",
    "$$Lf = Df - Wf = \\begin{pmatrix}d_1 & 0 & \\cdots & 0\\\\ 0 & d_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & d_{\\ell} \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} - \\begin{pmatrix}w_{11} & w_{12} & \\cdots & w_{1\\ell}\\\\ w_{12} & w_{22} & \\cdots & w_{2\\ell} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{\\ell 1} & w_{\\ell 2} & \\cdots & w_{\\ell\\ell} \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} = $$\n",
    "\n",
    "\n",
    "$$ = \\begin{pmatrix} d_1 \\\\ \\vdots \\\\ d_\\ell\\end{pmatrix} - \\begin{pmatrix} w_{11} + \\cdots + w_{1\\ell} \\\\ \\vdots \\\\ w_{\\ell 1} + \\cdots + w_{\\ell \\ell}\\end{pmatrix} = 0$$\n",
    "\n",
    "Положим, что существует соответствующий нулевому собственному значению собственный вектор $f' \\in \\mathbb{R}^n: $ $ \\exists p \\neq q $ такие, что $f'_p \\neq f'_q$ - т.е. он не является константным. Тогда $Lf' = 0 \\Rightarrow f'^T L f' = 0$. Поскольку мы рассматриваем связный граф, существует путь $p = i_0 \\to i_1 \\cdots \\to i_{n-1} \\to i_{n} = q$. Соседние вершины соединены ребром - отсюда следует, что $w_{i_j i_{j+1}} > 0$ - отсюда следует, что $f_{i_j} = f_{i_{j+1}}$ - иначе $f'^T L f > 0$. Отсюда же по цепочке выводится, что $f'_p = f'_q$ - противоречие. Значит $f'$ не будет собственным вектором для нулевого собственного значения, и мы доказали оба пункта.\n",
    "\n",
    "Теперь пусть $k>1$. Перенумеруем вершины в графе так, чтобы матрица $L$ приняла блочно-диагональный вид: \n",
    "\n",
    "$$L = \\begin{pmatrix} L_1 & 0 & \\cdots & 0 \\\\ 0 & L_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & 0 \\\\ 0 & 0 & \\cdots & L_k \\end{pmatrix} $$\n",
    "\n",
    "Каждый блок является лапласианом соответствующей компоненты связности графа, значит, нулевое собственное значение будет иметь кратность $k$, а собственные векторы $f_1,...,f_k$ будут задаваться по формуле из условия. Q.E.D.\n",
    "\n",
    "Кроме того, существует недоказанная, но проверенная эмпирически гипотеза. Она заключается в том, что для близких объектов $x_j, x_k$ (расстояние между ними мало) собственные векторы $f_i$, соответствующие малым собственным значениям, имеют близкие значения нужных координат: $f_{ij} \\approx f_{ik}$.\n",
    "\n",
    "Теперь мы можем сформулировать алгоритм спектральной кластеризации, основываясь на этой гипотезе:\n",
    "\n",
    "1. Строим по объектам граф $G$ и его лапласиан $L = D - W$.\n",
    "\n",
    "2. Находим нормированные собственные векторы $u_1,\\cdots,u_m$, соответствующие $m$ наименьшим собственным значениям.\n",
    "\n",
    "3. Составляем матрицу $U$, столбцы которой будут являться данными собственными векторами. По сути, мы переходим в новое признаковое пространство, где признаками являются собственные вектора.\n",
    "\n",
    "4. Обучаем на этой матрице K-Means с $k$ кластерами.\n",
    "\n",
    "В принципе, это можно рассматривать как общую технику преобразования признакового пространства.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Методы частичного обучения: self-training, моделирование смесью гауссиан, трансдуктивный SVM, графовый подход на основе лапласианов. \n",
    "\n",
    "Частичное обучение - это ~~учеба на пми~~ техника, при которой мы используем для обучения одновременно размеченные и неразмеченные данные. Есть несколько хороших способов это делать.\n",
    "\n",
    "Self-training: самый тупой подход. Обучаемся на размеченных данных, полученной моделью размечаем неразмеченные, добавляем в обучающую выборку, повторить. Можно добавлять самые уверенные предикты (все еще не гарантируется, что они верные!), можно добавлять все, можно добавлять все и вводить веса по уверенности предиктов.\n",
    "___\n",
    "\n",
    "Можно использовать смесь гауссиан для моделирования. Для простоты рассмотрим задачу бинарной классификации. Положим, у нас есть две выборки - размеченная $(X_\\ell, Y_\\ell)$ и неразмеченная $(X_u)$. Также у нас есть вероятностная модель с параметрами $\\Theta = (\\pi_0, \\pi_1, \\mu_0, \\mu_1, \\sigma_0, \\sigma_1)$ - априорные вероятности выбора компонент и их параметры распределений. В таком случае мы классифицируем объекты так:\n",
    "\n",
    "$$p(x,y|\\Theta) = p(y|\\Theta) p(x|y,\\Theta) = \\pi_y \\mathcal{N}(x; \\mu_y, \\sigma_y)$$\n",
    "\n",
    "$$p(y|x, \\Theta) = \\frac{p(x,y | \\Theta)}{\\sum_{y'} p(x,y' | \\Theta)}$$\n",
    "\n",
    "\n",
    "Правдоподобие размеченной выборки формулируется следующим образом:\n",
    "\n",
    "$$\\log p(X_\\ell,Y_\\ell|\\Theta) = \\sum_{i=1}^\\ell \\log p(y_i|\\Theta) p(x_i | y_i, \\Theta)$$\n",
    "\n",
    "Найти ОМП для $\\Theta$ в таком случае - тривиальная задача (доля класса, выборочные среднее и дисперсия).\n",
    "\n",
    "Правдоподобие неразмеченной выборки же формулируется так:\n",
    "\n",
    "$$\\log p(X_\\ell,Y_\\ell, X_u|\\Theta) = \\sum_{i=1}^\\ell \\log p(y_i|\\Theta) p(x_i | y_i, \\Theta) + \\sum\\limits_{i=\\ell+1}^{u} \\log \\left(\\sum_{y \\in \\{0,1\\}} p(y|\\Theta) p(x_i | y_i, \\Theta) \\right)$$\n",
    "\n",
    "Такую херню ОМП уже не пробьет, придется воспользоваться ЕМ-алгоритмом.\n",
    "\n",
    "Начнем с ОМП по размеченной выборке для $\\Theta$ - обозначим за $\\Theta_0$.\n",
    "\n",
    "На Е-шаге мы будем считать наиболее вероятную разметку для всех новых данных $x \\in X_u$ как $$p(y|x,\\Theta_{old}) = \\frac{p(x,y | \\Theta_{old})}{\\sum_{y'} p(x,y' | \\Theta_{old})}$$\n",
    "\n",
    "На М-шаге мы обновляем ОМП по теперь уже доразмеченной выборке - используем пропорции классов, выборочные среднее и дисперсию.\n",
    "\n",
    "По сути, это почти то же самое, что self-training.\n",
    "\n",
    "____\n",
    "\n",
    "Transductive SVM (TSVM) или Self-Supervised SVM (S3VM) - алгоритм для частичного обучения, связанный с обычным SVM. Но если обычный SVM максимизирует ширину разделяющей полосы на обучающей выборке, то S3VM максимизирует ее ширину на неразмеченной выборке. Идея такая: строим классификатор для всех $2^u$ разметок на новой выборке и выбираем тот, ширина разделяющей полосы которого максимальна. \n",
    "\n",
    "Обозначим $f(x) = \\langle w, x \\rangle + b$, $x_+ = \\text{ReLU}(x) = \\max(0,x)$. Вспомним, что для SVM ответ на объекте рассчитывается как $\\text{sign} (f(x))$. А значит, для неразмеченных данных можно заменить произведение ответа на таргет на модуль ответа. Собственно, оптимизируемый функционал: $$\\sum\\limits_{i=1}^{\\ell} (1 - y_i f(x_i))_+ + \\lambda_1 ||w||_2^2 + \\lambda_2 \\sum\\limits_{i=\\ell+1}^u (1 - |f(x_i)|)_+ \\to \\min\\limits_{w,b} $$\n",
    "\n",
    "Hat Loss $(1 - |f(x_i)|)_+$ устроен так, что нам для минимизации надо сделать ответ на объекте большим 1 - т.е. максимально отдалить его от разделяющей полосы.\n",
    "\n",
    "В общем-то, алгоритм такой: мы оптимизируем приведенный выше функционал, при этом используем ограничение $$\\frac{1}{u-\\ell} \\sum\\limits_{i=\\ell+1}^u f(x_i) = \\frac{1}{\\ell} \\sum_{i=1}^{\\ell} y_i$$ для того, чтобы сохранить адекватный баланс классов в новых данных (без этого чаще всего S3VM падает к решению с подавляющим числом объектов одного класса - т.е. тупо находит выброс). Дальше объект $x$ классифицируется как $\\text{sign} (f(x))$.\n",
    "\n",
    "___\n",
    "\n",
    "Также можно использовать связанный со спектральной кластеризацией подход. Построим граф $G$ по выборке $X_\\ell \\cup X_u$. Доразметить данные можно с помощью так называемой гармонической функции $f(x_i)$, удовлетворяющей следующим свойствам:\n",
    "\n",
    "1. $f(x_i) = y_i$ для всех объектов обучающей выборки\n",
    "\n",
    "2. $f$ минимизирует \"энергию\" графа: $\\sum\\limits_{i,j=1}^n w_{ij} \\left(f(x_i) - f(x_j)\\right)^2$ \n",
    "\n",
    "3. Ответ на объекте неразмеченной выборки равен среднему ответу по его соседям: $f(x_i) = \\frac{\\sum_{w_ij \\neq 0} w_{ij} f(x_j)}{\\sum_{w_ij \\neq 0} w_{ij}}$\n",
    "\n",
    "$f$ в целом можно подобрать тупым алгоритмом - инициализируем значения на обучающих объектах верными лейблами и гоняем до сходимости на неразмеченной выборке третье условие, т.е. на каждом шаге пересчитываем ответ как среднее по соседям. \n",
    "\n",
    "Но так делать не очень интересно, поэтому выпишем closed-form решение. Если составить лапласиан $L$ графа $G$, то мы можем переписать выражение для \"энергии\" графа как $f^T L f$ - в данном произведении $f$ является вектором значений функции на всех объектах. \n",
    "\n",
    "Теперь нам нужно, используя условия выше, минимизировать по $f$ следующее выражение: \n",
    "\n",
    "$$\\infty \\cdot \\sum\\limits_{i=1}^{\\ell} \\left(f(x_i) - y_i\\right)^2 + f^T L f$$\n",
    "\n",
    "Оказывается, здесь реально существует closed-form решение. \n",
    "\n",
    "Запишем лапласиан в виде блочной матрицы для объединенной выборки: \n",
    "\n",
    "$$L = \\begin{pmatrix} L_{\\ell \\ell} & L_{\\ell u} \\\\ L_{u \\ell} & L_{uu} \\end{pmatrix}$$\n",
    "\n",
    "В таком случае гармоническое решение \n",
    "\n",
    "$$f_u = -L_{uu}^{-1} L_{u \\ell} Y_\\ell$$ \n",
    "\n",
    "Я бы очень хотел посмотреть на пруф, но к сожалению авторы его не привели( надеюсь, на колоке желания смотреть на него ни у кого не возникнет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Несбалансированная классификация: undersampling, oversampling, SMOTE.\n",
    "\n",
    "Чилловый билет.\n",
    "\n",
    "Несбалансированная классификация - название говорит само за себя. Решаем такую задачу, где соотношение классов является очень несбалансированным. К примеру, по признаковому описанию человека детектим у него наличие очень редкого спидорака.\n",
    "\n",
    "Undersampling - будем просто удалять объекты доминирующего класса до тех пор, пока соотношение не станет нормальным.\n",
    "\n",
    "Oversampling - займемся обратным действием и будем клонировать объекты минорного класса (можно подобавлять эпсилоны, в разумных пределах), опять же пока соотношение не станет приемлемым. По сути, это примерно аналогично усиленному взвешиванию объектов минорного класса.\n",
    "\n",
    "SMOTE - здесь мы генерим синтетические объекты минорного класса. Для объекта $x_1$ выбираем $k$ ближайших соседей, из этих соседей берем случайного $x_2$ и создаем новый объект как выпуклую комбинацию текущих: $x_{new} = \\alpha x_1 + (1 - \\alpha) x_2, \\alpha \\in (0; 1)$. Здесь по идее надо следить за распределением объектов, может получиться так, что мы создадим что-то близкое к примеру из доминирующего класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Одноклассовая классификация: непараметрические методы (восстановление плотности), одноклассовый SVM, Isolation forest.\n",
    "\n",
    "Задача одноклассовой классификации состоит в том, что мы учимся детектить выбросы. По сути, это экстремальный случай задачи несбалансированной классификации.\n",
    "\n",
    "Начать стоит со статистических методов. В таком подходе мы восстанавливаем плотность выборки и затем по этой плотности учимся предсказывать \"аномальность\" объекта. К примеру, можно отсекать по порогу значение плотности на этом объекте или расстояние от этого объекта до среднего в распределении, этот порог можно считать настраиваемым параметром - подгоняем его под известные аномалии.\n",
    "\n",
    "Для восстановления плотности можно использовать как параметрические, так и непараметрические методы. Рассмотрим в первую очередь второй подход.\n",
    "\n",
    "Для одномерных величин нам нужно сначала вспомнить определение плотности. Зададим через предел: \n",
    "\n",
    "$$p(x) = \\lim\\limits_{h \\to 0} \\frac{1}{2h} \\mathbb{P} (\\xi \\in [x-h; x+h])$$\n",
    "\n",
    "По этому определению можно построить эмпирическую оценку плотности:\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{2h} \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell \\left[|x-x_i| < h\\right] = \\frac{1}{\\ell h}  \\sum\\limits_{i=1}^\\ell \\frac{1}{2} \\left[\\frac{|x-x_i|}{h} < 1\\right]$$ \n",
    "\n",
    "h здесь будет регулирующим гладкость параметром. Однако, такая плотность за счет использования индикаторных величин является негладкой. Поэтому заменим индикатор на некоторую гладкую функцию $K(x)$.\n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{\\ell h}  \\sum\\limits_{i=1}^\\ell \\frac{1}{2} K \\left( \\frac{x - x_i}{h} \\right)$$\n",
    "\n",
    "Здесь $K(x)$ - ядро, которое должно удовлетворять следующим свойствам:\n",
    "\n",
    "1. $K(-x) = K(x)$ - четность.\n",
    "\n",
    "2. $\\int K(x) dx = 1$ - нормированность.\n",
    "\n",
    "3. $K(x) \\geq 0$ - неотрицательность.\n",
    "\n",
    "4. Невозрастание при $x > 0$. \n",
    "\n",
    "Таким свойствам удовлетворяет, к примеру, гауссово ядро: $$K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}$$\n",
    "\n",
    "Теперь мы можем обобщить это на случай с многомерным распределением, достаточно заменить $|x-x_i|$ на некоторую метрику $\\rho(x, x_i)$: \n",
    "\n",
    "$$\\hat{p}(x) = \\frac{1}{\\ell V(h)}  \\sum\\limits_{i=1}^\\ell \\frac{1}{2} K \\left( \\frac{\\rho(x,x_i)}{h} \\right)$$\n",
    "\n",
    "здесь $V(h) = \\int K\\left(\\frac{\\rho(x, x_i)}{h} \\right) dx$ - нормировочная константа. Данный класс методов подходит только для маломерных пространств, поскольку число объектов, необходимых для нормальной оценки плотности, растет экспоненциально с ростом числа признаков. \n",
    "\n",
    "Также можно использовать параметрический подход - здесь мы подгоняем распределение $p(x|\\theta)$ из какого-то класса параметрических распределений (e.g. нормальное, смесь нормальных) под выборку с помощью ОМП на $\\theta$. \n",
    "\n",
    "Кроме статистических методов, можно также воспользоваться метрическими методами. Здесь мы сделаем такое предположение, что выбросы лежат далеко от основных кластеров выборки, поэтому будем считать объект $x$ аномальным, если $p$ или менее процентов объектов выборки находятся от него на расстоянии не более $\\epsilon$:\n",
    "\n",
    "$$\\frac{1}{\\ell}\\sum\\limits_{i=1}^\\ell [\\rho(x, x_i) \\leq \\epsilon] \\leq p$$\n",
    "\n",
    "Пороги $p, \\epsilon$ здесь являются настраиваемыми параметрами.\n",
    "\n",
    "Кроме того, аномалии можно распознавать с помощью каких-то классических методов. Скажем, мы можем настроить классификатор так, чтобы на нормальных объектах он принимал специфические значения - околонулевые, положительные, етц. Тогда если нам придет объект, прогноз на котором сильно отличается, то мы можем сказать, что он аномальный.\n",
    "\n",
    "По сути, нам нужно построить модель, которая будет давать ответ 1 в как можно меньшей области, содержащей как можно больше нормальных объектов, и 0 во всех остальных точках. Это можно сделать с использованием SVM.\n",
    "\n",
    "Будем строить линейную модель классификации $a(x) = \\text{sign}\\langle w,x \\rangle$ и потребуем, чтобы она отделяла выборку от начала координат с максимальным отступом. Получаем следующую оптимизационную задачу:\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{1}{2} ||w||_2^2  + \\frac{1}{\\nu \\ell } \\sum\\limits_{i=1}^\\ell \\xi_i - \\rho \\to \\min\\limits_{\\xi, w, \\rho}\\\\\n",
    "\\\\\n",
    "\\langle w, x_i \\rangle \\geq \\rho - \\xi_i, & i \\in \\{1,...,\\ell\\} \\\\\n",
    "\\\\\n",
    "\\xi_i \\geq 0, & i \\in \\{1,...,\\ell\\} \\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Здесь гиперпараметр $\\nu$ отвечает за корректность на обучающей выборке - по факту он является верхней границе на число объектов выборки, для которых $a(x) = -1$ - аномалий. Предсказание получаем так: $$a(x) = \\text{sign}(\\langle w, x \\rangle - \\rho)$$\n",
    "\n",
    "Получается, мы строим такую гиперплоскость, что\n",
    "\n",
    "1. Она отделяет от нуля как можно больше объектов выборки - за это отвечает слагаемое $\\frac{1}{\\nu \\ell } \\sum\\limits_{i=1}^\\ell \\xi_i$\n",
    "\n",
    "2. Она имеет как можно большую ширину $\\frac{1}{||w||^2_2}$.\n",
    "\n",
    "3. Она имеет как можно больший отступ от 0 - за это отвечает максимизация $\\rho$.\n",
    "\n",
    "Как и в обычном SVM, здесь можно сделать двойственную задачу и ядровой переход: \n",
    "\n",
    "\\begin{align}\n",
    "\\begin{cases}\n",
    "\\frac{1}{2} \\sum\\limits_{i,j=1}^\\ell \\lambda_i \\lambda_j K(x_i, x_j) \\to \\min_\\lambda\n",
    "\\\\\n",
    "0 \\leq \\lambda_i \\leq \\frac{1}{\\nu \\ell},& i \\in \\{1,...,\\ell\\} \\\\\n",
    "\\\\\n",
    "\\sum\\limits_{i=1}^\\ell \\lambda_i = 1 \\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Тогда классификатор принимает вид: $$a(x) = \\text{sign}\\left(\\sum\\limits_{i=1}^\\ell \\lambda_i K(x, x_i) - \\rho\\right)$$\n",
    "\n",
    "___\n",
    "\n",
    "Также в данной задаче можно использовать RF. Вспомним, что этот алгоритм можно рассматривать как задающий функцию расстояния - два объекта считаются похожими, если они часто попадают в один лист. \n",
    "\n",
    "Построим лес из $N$ деревьев. Каждое дерево будем строить стандартным алгоритмом, но при этом признак и порог на каждый сплит будем выбирать случайно. Строить будем, пока не доберемся до вершины с одним объектом в ней, либо пока не достигнем максимальной высоты дерева - можно положить ее равной $\\log_2 \\ell$. Здесь мы делаем предположение о том, что аномальный объект может быть быстрее отделен случайными сплитами от выборки, поэтому он с большой вероятностью будет оказываться на небольшой глубине дерева. \n",
    "\n",
    "Для оценки аномальности объекта найдем расстояние от соответствующего ему листа до корня в каждом дереве. При этом, если в $n$-ом дереве лист содержит только один объект, то будем брать в качестве оценки $h_n(x)$ собственно высоту листа $k$, а если же он содержит $m>1$ объектов, то будем брать $h_n(x)= k + c(m)$, где $c(m)$ - средняя длина пути от корня до листа в бинарном дереве поиска: $$c(m) = 2H(m-1) - 2\\frac{m-1}{m}$$\n",
    "$H(i) \\approx ln(i) + 0.5772156649$ - i-ое гармоническое число. Оценку аномальности вычислим с помощью средней глубины, нормированной на среднюю длину пути в дереве: $$a(x) = 2^{-\\frac{\\frac{1}{N} \\sum_n^N h_n(x)}{c(\\ell)}} $$\n",
    "\n",
    "Можно считать деревья по подвыборкам $s$, чтобы было побыстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Модели со скрытыми переменными (latent factor model, LFM) для построения рекомендаций. Обучение LFM: стохастический градиентный спуск, ALS, HALS.\n",
    "\n",
    "Будем строить для каждого юзера $u$ и айтема $i$ векторы $p_u, q_i \\in \\mathbb{R}^d$, которые будут эмбедингами \"категорий интересов\". По идее, компоненты юзера можно интерпретировать как степень заинтересованности в категории, а компоненты айтема как степень принадлежности к категории. Впрочем, никто не гарантирует, что эти компоненты будут соответствовать каким-то реальным категориям.\n",
    "\n",
    "Сходство юзера и айтема будем оценивать скалярным произведением эмбедингов: $$r_{ui} \\approx \\langle p_u, q_i \\rangle$$\n",
    "\n",
    "Также скалярным произведением можно оценить сходство двух юзеров или двух айтемов.\n",
    "\n",
    "Для данной задачи мы можем сформулировать следующий функционал ошибки: \n",
    "\n",
    "$$\\sum_{(u,i) \\in R} \\left(r_{ui} - \\overline{r}_u - \\overline{r}_i - \\langle p_u, q_i \\rangle\\right)^2 \\to \\min_{P,Q}$$\n",
    "\n",
    "Здесь мы суммируем по всем парам $(u,i)$, для которых известен рейтинг. Заметим, что если матрица $R'$ - $R$ с центрированными строками и столбцами, то задача сводится к низкоранговому матричному разложению: $$||R' - P^T Q||_2^2 \\to \\min_{P,Q}$$\n",
    "\n",
    "Здесь представления юзеров и айтемов записаны в матрицах $P$ и $Q$. \n",
    "\n",
    "Можно домножать скалярные произведения на масштабирующий множитель $\\alpha$: $$||R' - \\alpha P^T Q||_2^2 \\to \\min_{\\alpha, P,Q}$$\n",
    "\n",
    "Можно регуляризировать эмбединги:\n",
    "\n",
    "$$\\sum_{(u,i) \\in R} \\left(r_{ui} - \\overline{r}_u - \\overline{r}_i - \\langle p_u, q_i \\rangle\\right)^2 + \\lambda \\sum_u ||p_u||^2 + \\mu \\sum_i ||q_i||^2 \\to \\min_{P,Q}$$\n",
    "\n",
    "Собственно, это и есть LFM.\n",
    "\n",
    "___\n",
    "\n",
    "Оптимизировать это дело можно разными способами. Самый тупой подход - SGD. На каждом шаге будем выбирать случайную пару $(u,i)$ и делать следующий шаг:\n",
    "\n",
    "$$p_{uk} := p_{uk} + \\eta q_{ik} \\left(r_{ui} - \\overline{r}_u - \\overline{r}_i - \\langle p_u, q_i \\rangle\\right)$$\n",
    "\n",
    "$$q_{ik} := q_{ik} + \\eta p_{uk} \\left(r_{ui} - \\overline{r}_u - \\overline{r}_i - \\langle p_u, q_i \\rangle\\right)$$\n",
    "\n",
    "Но можно придумать более флексовый подход. Оказывается, что оптимизируемый функционал, хоть и не является выпуклым в совокупности по $P$ и $Q$, становится таким, если мы фиксируем $P$ либо $Q$. Более того, мы можем выписать аналитическое решение для $P$ при фиксированном $Q$ и наоборот:\n",
    "\n",
    "$$p_u = \\left( \\sum\\limits_{i: \\exists r_{ui}} q_i q_i^T\\right)^{-1} \\sum\\limits_{i: \\exists r_{ui}} r_{ui} q_i$$\n",
    "\n",
    "$$q_i = \\left( \\sum\\limits_{u: \\exists r_{ui}} p_u p_u^T\\right)^{-1} \\sum\\limits_{u: \\exists r_{ui}} r_{ui} p_u$$\n",
    "\n",
    "Здесь $p_u, q_i$ - столбцы матриц $P, Q$. Этот метод называется ALS.\n",
    "\n",
    "Как нам известно, обращение матрицы в вычислительных кругах считается дурным тоном, поэтому стоит постараться от него избавиться. Это действительно можно сделать, если не фиксировать всю матрицу $P(Q)$, а оставлять незафиксированной одну строку $p_k(q_k)$. Тогда можно записать оптимальное решение для этой строки:\n",
    "\n",
    "$$p_k = \\frac{q_k\\left(R - \\sum\\limits_{s \\neq k} p_s q_s^T\\right)^T }{q_k q_k^T}$$\n",
    "\n",
    "$$q_k = \\frac{p_k\\left(R - \\sum\\limits_{s \\neq k} p_s q_s^T\\right) }{p_k p_k^T}$$\n",
    "\n",
    "Этот метод называется HALS. Отметим, что как и в градиентном спуске, данные обновления нужно гонять до сходимости модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Неявная информация в рекомендательных системах, implicit ALS.\n",
    "\n",
    "В рекомендательных системах интерес юзера к айтему может быть как явным (скажем, поставленный лайк), так и неявным (скажем, просто просмотр страницы). Неявной информации чаще всего гораздо больше - и, хоть она несет гораздо меньше полезных знаний, отбрасывать ее тоже не очень хочется. \n",
    "\n",
    "Один из способов ее учета - метод Implicit ALS (iALS). Введем показатель неявного интереса юзера к айтему:\n",
    "\n",
    "$$s_{ui} = \\begin{cases} 1, & \\exists r_{ui} \\\\ 0, & иначе \\end{cases}$$\n",
    "\n",
    "Здесь мы сделали предположение, что даже низкая оценка юзером айтема является более полезной, чем ее отсутствие. При этом отсутствие оценки не является показателем отсутствия интереса - возможно, юзер просто не нашел айтем. Поэтому стоит ввести веса $c_{ui}$, характеризующие уверенность в показателе интереса $s_{ui}$: $$c_{ui} = 1 + \\alpha r_{ui}$$\n",
    "\n",
    "Здесь $\\alpha$ позволяет регулировать влияние проставленного рейтинга на степень заинтересованности.\n",
    "\n",
    "Теперь мы можем задать функционал: \n",
    "\n",
    "$$\\sum\\limits_{(u,i) \\in D} c_{ui} \\left(s_{ui} - \\overline{s}_u - \\overline{s}_i - \\langle p_u, q_i \\rangle\\right)^2 + \\lambda \\sum_u ||p_u||^2 + \\mu \\sum_i ||q_i||^2 \\to \\min_{P,Q}$$\n",
    "\n",
    "Заметим, что теперь сумма идет по всем парам юзер-айтем, а не только по тем, для которых имеется рейтинг. \n",
    "\n",
    "Обучать это можно вышеупомянутыми методами - SGD, (H)ALS. Веса $c$ и показатели интереса $s$ можно также вычислять по-разному в зависимости от задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Факторизационные машины. FFM. Способы обучения моделей.\n",
    "\n",
    "Допустим, мы захотели решить задачу регрессии с использованием произведений признаков второго порядка:\n",
    "\n",
    "$$a(x) = w_0 + \\sum_j^d w_j x_j + \\sum_{j_1}^d \\sum_{j_2 > j_1}^dw_{j_1} w_{j_2} x_{j_1} x_{j_2}$$\n",
    "\n",
    "Здесь возникает очевидная проблема - нам придется хранить $O(d^2)$ весов. Если у нас есть, к примеру, категориальные признаки с большим числом категорий, модель может стать слишком неподъемной. Предположим, что вес взаимодействия признаков $j_1, j_2$ может быть аппроксимирован скалярным произведением скрытых векторов $v_{j_1}, v_{j_2}$. В таком случае модель примет вид:\n",
    "\n",
    "$$a(x) = w_0 + \\sum_j^d w_j x_j + \\sum_{j_1}^d \\sum_{j_2 > j_1}^d \\langle v_{j_1}, v_{j_2} \\rangle x_{j_1} x_{j_2}$$\n",
    "\n",
    "Такая модель гораздо лучше, поскольку теперь нам придется хранить $O(rd)$ параметров, где $r$ - размерность скрытых векторов.\n",
    "\n",
    "Задачу рекомендаций можно представить как регрессию по двум категориальным признакам - id юзера и айтема. Для некоторых пар мы знаем рейтинг, для остальных пытаемся его восстановить. В сущности, объект будет иметь признаковое описание длины $|U| + |I|$ - это будут два one-hot вектора. В таком случае факторизационная машина будет иметь вид:\n",
    "\n",
    "$$a(x) = w_0 + w_u + w_i + \\langle v_u, v_i \\rangle$$ \n",
    "\n",
    "Такая форма полностью соответствует исходному функционалу. \n",
    "\n",
    "Заметим, что в такой модели у каждого признака имеется всего один скрытый вектор, отвечающий за взаимодействие с другими признаками. Но мы можем расширить такую модель, введя для каждого признака векторы для взаимодействия с разными группами. Тогда модель примет вид: \n",
    "\n",
    "$$a(x) = w_0 + \\sum_j^d w_j x_j + \\sum_{j_1}^d \\sum_{j_2 > j_1}^d \\langle v_{j_1, f_{j_2}}, v_{j_2, f_{j_1}} \\rangle x_{j_1} x_{j_2}$$\n",
    "\n",
    "Здесь $f_{j_1}, f_{j_2}$ - индексы групп признаков $j_1, j_2$. Данная модель называется Field-aware Factorization Machine, FFM.\n",
    "\n",
    "___\n",
    "\n",
    "Формализуем задачу обучения факторизационной машины:\n",
    "\n",
    "$$Q(a, X) = \\sum\\limits_{i=1}^\\ell L(a(x_i; \\Theta), y_i) + \\sum\\limits_{j=1}^{|\\Theta|} \\lambda_j \\theta_j^2 \\to \\min_{\\Theta}$$\n",
    "\n",
    "Здесь $\\Theta$ - параметры модели (непосредственные веса признаков + скрытые векторы каждого), $\\lambda_j$ - коэффициенты регуляризаций этих параметров, $L$ - функция потерь (можно брать MSE для регрессии или логлосс для классификации).\n",
    "\n",
    "Поговорим о том, как это обучать. Есть три основных подхода - SGD (оптимизируем параметры сгдшкой по выборке), ALS (оптимизируем по очереди по каждому параметру при фиксированных остальных, можно даже использовать аналитические оптимумы), MCMC (сэмплируем из апостериорного распределения)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Метрики качества рекомендаций.\n",
    "\n",
    "В задаче мы учимся предиктить рейтинги $r_{ui}$, собственно, качество таких предиктов мы и будем оценивать. \n",
    "\n",
    "Если мы предсказываем рейтинг или что-то другое вещественное (e.g. длительность нахождения на странице), логично будет оценивать качество метриками регрессии - MSE, MAE,...\n",
    "\n",
    "Если мы предсказываем вероятность дискретного события (клик, покупка, etc), логично оценивать качество метриками классификации - аккураси, пресижн, реколл, ауки...\n",
    "\n",
    "Также можно учесть, что мы показываем юзеру только $k$ самых лучших айтемов, и нас интересуют метрики, оцененные по этим айтемам. Обозначим за $R_u(k)$ лучшие $k$ товаров для юзера, а за $L_u$ - товары, для которых интересующее нас событие действительно произошло. В таком случае можно ввести следующие метрики:\n",
    "\n",
    "1. hitrate@k $ = [ R_u(k) \\cap L_u \\neq \\varnothing ]$ - наличие верной рекомендации.\n",
    "\n",
    "2. precision@k $ = \\frac{|R_u(k) \\cap L_u|}{|R_u(k)|}$ - доля заинтересовавших юзера предсказаний среди всех предсказаний.\n",
    "\n",
    "3. recall@k $ = \\frac{|R_u(k) \\cap L_u|}{|L_u|}$ - доля заинтересовавших юзера предсказаний среди всего, что ему понравилось.\n",
    "\n",
    "Заметим, что по факту нам нужно не предиктить точечные оценки рейтингов, а всего лишь давать более уверенные предсказания релевантным айтемам. Поэтому имеет смысл использовать те же метрики, что и в задаче ранжирования. Одной из таких метрик является nDCG (discounted cumulative gain).\n",
    "\n",
    "Обозначим за $a_{ui}$ предикт модели для пары $(u,i)$. Отсортируем все товары для юзера по убыванию этого предикта. Тогда для айтема $i_p$ на позиции $p$ можно вычислить его полезность $g(r_{ui_p})$ и штраф за позицию $d(p)$. Метрика задается так:\n",
    "\n",
    "$$DCG@K(u) = \\sum_{p=1}^k g(r_{ui_p}) d(p)$$\n",
    "\n",
    "В качестве конкретных функций можно взять $g(r) = 2^r - 1$ и $d(p) = \\frac{1}{\\log(p+1)}$. Для удобства интерпретации можно делить метрику на максимально возможное значение:\n",
    "\n",
    "$$nDCG@K(u) = \\frac{DCG@K(u)}{\\max DCG@K(u)}$$\n",
    "\n",
    "___\n",
    "\n",
    "Заметим, что все наши метрики на текущий момент оценивают лишь качество рекомендации, но никак не смотрят на работу самой рекомендательной системы. Чтобы это исправить, можно учитывать следующие вещи.\n",
    "\n",
    "Стоит смотреть, какая доля каталога вообще показывается юзерам. Логично, что мы хотели бы рекомендовать весь ассортимент, а не только айтемы из какого-то ограниченного пула. Проще всего это сделать, посчитав покрытие каталога - долю товаров, которые были показаны хотя бы один раз. Для оценки общего разнообразия показов можно посчитать энтропию распределения доли показов каждого товара $p(i)$:\n",
    "\n",
    "$$H(p) = -\\sum_{i \\in I} p_i \\log(p_i)$$\n",
    "\n",
    "Также полезно смотреть, какой доле пользователей вообще не показываются товары - это поможет устранять возможные проблемы с рекомендациями, например, для пользователей с низкой уверенностью предиктов.\n",
    "\n",
    "Логично также смотреть, о каких товарах пользователь смог впервые узнать на нашем сайте благодаря рекомендациям. Для измерения новизны рекомендаций можно использовать следующие подходы:\n",
    "\n",
    "1. Добавить возможность сообщить о том, что этот рекомендуемый товар юзер уже видел.\n",
    "\n",
    "2. Удалить из выборки часть товаров, которые пользователь уже купил или посмотрел. Так мы моделируем ситуацию, при которой юзер где-то раньше узнал про этот товар на другом сайте. Далее будем оценивать новизну на основе того, как часто эти товары попадают в рекомендации.\n",
    "\n",
    "3. Можно предположить, что юзер чаще видел популярные айтемы, чем непопулярные. Тогда можно считать новизну на основе доли угаданных товаров с весами, обратно пропорциональными их популярности.\n",
    "\n",
    "\n",
    "Еще одной полезной характеристикой модели является ее прозорливость, т.е. способность давать радикально новые для юзера и при этом полезные рекомендации. Ее можно измерять как долю рекомендаций, далеких от всех просмотренных юзером айтемов. \n",
    "\n",
    "Рассмотрим на примере - мы рекомендуем книгу нового автора юзеру, который раньше рейтил только книги от одного автора. Допустим, мы хотим измерить расстояние $d(b, B)$ между новой книгой $b$ и множеством уже измеренных книг $B$. Обозначим через $C_{B,w}$ число книг автора $w$ в множестве $B$, а через $C_B$ - максимальное количество книг одного автора в $B$. Тогда расстояние можно определить так: $$d(b, B) = \\frac{1 + C_B - C_{B, w(b)}{1 + C_B}$$ где $w(b)$ - автор книги b.\n",
    "\n",
    "Еще нам хотелось бы рекомендовать не 10 одинаковых товаров разного цвета, а все-таки 10 разных. Поэтому стоит отслеживать разнообразность предлагаемых товаров. Можно, к примеру, учитывать среднее попарное расстояние между всеми товарами в рекомендуемом наборе. Расстояние можно считать как по удаленности категорий в каталоге, так и, к примеру, по совпадению множеств купивших их пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
